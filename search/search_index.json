{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>CLEESE is a Python toolbox to help the generation of randomized sound and image stimuli for neuroscience research. </p> <p>It provides a number of sound and image transformation algorithms (so-called <code>Engines</code>) able e.g. to create natural-sounding expressive variations around an original speech recording, or expressive variations on a human face. It also provides a config file interface to automatize the call to these algorithms, in order e.g. to easily create thousands of random variants from a single base file, which can serve as stimuli for neuroscience experiments. </p> <p>As of version v2.0.0, CLEESE is composed of two engines: <code>PhaseVocoder</code> and <code>Mediapipe</code>: </p> <ul> <li> <p><code>PhaseVocoder</code> allows one to create random fluctuations around an audio file\u2019s original contour of pitch, loudness, timbre and speed (i.e. roughly   defined, its prosody). One of its foreseen applications is the generation of random voice stimuli for reverse correlation experiments in the vein of Ponsot, Burred, Belin &amp; Aucouturier (2018) Cracking the social code of speech prosody using reverse correlation. PNAS, 115(15), 3972-3977.</p> </li> <li> <p><code>FaceWarp</code> uses mediapipe's Face Mesh API to introduce random or precomputed deformation in the expression of a   visage on an image. This engine was designed to produce batches of deformed faces for reverse correlation experiments in the vein of Jack, Garrod, Yu, Caldara, &amp; Schyns (2012). Facial expressions of emotion are not culturally universal. PNAS, 109(19), 7241-7244.</p> </li> </ul>"},{"location":"about/","title":"About CLEESE","text":"<p>CLEESE is a free, standalone Python module, distributed under an open-source MIT Licence on the FEMTO Neuro team github page. </p> <p>It was originally designed in 2018 by Juan Jos\u00e9 Burred, Emmanuel Ponsot and Jean-Julien Aucouturier at STMS Lab (IRCAM/CNRS/Sorbonne Universit\u00e9, Paris - France), and released on the IRCAM Forum plateform. As of 2021, CLEESE is now developped and maintained by the FEMTO Neuro Team at the FEMTO-ST Institute (CNRS/Universit\u00e9 Bourgogne Franche-Comt\u00e9) in Besan\u00e7on - France, and distributed on the team's github page. </p> <p>CLEESE's development was originally funded by the European Research Council (CREAM 335536, 2014-2019, PI: JJ Aucouturier), and has since then received support from Agence Nationale de la Recherche (ANR SEPIA, AND Sounds4Coma), Fondation pour l'Audition (DASHES) and R\u00e9gion Bourgogne-Franche Comt\u00e9 (ASPECT). </p>"},{"location":"about/#cleese-contributors","title":"CLEESE contributors:","text":"<ul> <li>Juan Jos\u00e9 Burred (original development, Phase Vocoder Engine)  jjburred</li> <li>Emmanuel Ponsot (tool specification) </li> <li>JJ Aucouturier (software architecture)  jjau</li> <li>Lara Kermarec (Face warp engine)  nemirwen</li> <li>Paige Tuttosi (Documentation)  chocobearz</li> </ul>"},{"location":"api/face-warp/","title":"Face Warp","text":"<p>CLEESE's <code>FaceWarp</code> engine works by first identifying a set of landmarks on the visage present in the image. Then a gaussian distribution of deformation vectors is applied to a subset of landmarks, and the Moving Least Squares (MLS) algorithm is used to apply the deformation to the image itself. Alternatively, a precomputed set of deformations can also be provided.</p>"},{"location":"api/face-warp/#modes-of-operation","title":"Modes of operation","text":"<p>CLEESE can be used in different modes, depending on which function you call and how. </p>"},{"location":"api/face-warp/#batch-generation","title":"Batch generation","text":"<p>CLEESE has a dedicated function for batch treatments: <code>cleese.generate_stimuli</code>, which is used, in <code>Mediapipe</code>'s case, to generate a number of randomly deformed visages.</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\ncleese.generate_stimuli(Mediapipe, inputFile, configFile)\n</code></pre> <p>Two parameters have to be set by the user:</p> <ul> <li><code>inputFile</code>: the path to the base image, which can be any image format readable and writeable by <code>PIL</code>.</li> <li><code>configFile</code>: the path to the configuration script</li> </ul> <p>All the generation parameters for all treatments are set up in the configuration script that has to be edited or created by the user. An example of configuration script with parameters for all treatments is included with the toolbox: <code>cleese-mediapipe.toml</code>. Configuration parameters will be detailed below.</p> <p>For each run in batch mode, the toolbox generates the following folder structure, where <code>&lt;outPath&gt;</code> is specified in the parameter file:</p> <ul> <li><code>&lt;outPath&gt;/&lt;currentExperimentFolder&gt;</code>: main folder for the current generation experiment. The name <code>&lt;currentExperimentFolder&gt;</code> is automatically created from the current date and time. This folder contains:<ul> <li><code>&lt;baseImage.ext&gt;</code>: a copy of the base image used for the current experiment</li> <li><code>*.toml</code>: a copy of the configuration script used for the current experiment</li> <li><code>&lt;baseimage&gt;.xxxxxxxx.&lt;ext&gt;</code>: the generated deformed image, where <code>xxxxxxxx</code> is a running number (e.g.: <code>monalisa.00000001.jpg</code>)</li> <li><code>&lt;baseimage&gt;.xxxxxxxx.dfmxy</code>: the generated deformation vectors, in CSV format, for the generated stimulus (e.g.: <code>monalisa.00000001.dfmxy</code>)</li> <li><code>&lt;baseimage&gt;.landmarks.txt</code>: the list of all landmarks positions as detected on the original image, in ASCII format, readable with <code>numpy.loadtxt</code>.</li> </ul> </li> </ul>"},{"location":"api/face-warp/#array-input-and-output","title":"Array input and output","text":"<p>CLEESE can also provide a single result, based on data loaded previously in a script or directly loading a file.</p> <p>From array: </p> <p>Here, you can also use <code>PIL.Image</code>'s <code>np.array(Image.open(\"path/image.jpg\").convert(\"RGB\"))</code> to load the image. The <code>Face Mesh</code> API requires that the image be in RGB format.</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\nimg = Mediapipe.load_file(inputFile)\ndeformedImg = cleese.process_data(Mediapipe, img, configFile)\n</code></pre> <p>From file: <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\ndeformedImg = cleese.process_file(Mediapipe, inputFile, configFile)\n</code></pre></p> <p>In both of those cases, no files or folder structures are generated.</p>"},{"location":"api/face-warp/#applying-a-deformation","title":"Applying a deformation","text":"<p>Both <code>cleese.process_data</code> and <code>cleese.process_file</code> function allow for applying a precomputed set of deformations instead of generating them randomly. CLEESE can accept both <code>.dfmxy</code> deformations (absolute cartesian deformation vectors), and <code>.dfm</code> deformations (deformation vectors mapped onto a triangulation of face landmarks (dlib landmarks indices)).</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ndfmxyFile = 'path_to_dfmxy.dfmxy'\ndfmFile = 'path_to_dfm.dfm'\nimageFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\n# .dfmxy processing\ndfmxy = Mediapipe.load_dfmxy(dfmxyFile)\nimg = cleese.process_file(Mediapipe,\n                          imageFile,\n                          configFile,\n                          dfmxy=dfmxy)\n\n# .dfm processing\ndfm = Mediapipe.load_dfm(dfmFile)\nimg = cleese.process_file(Mediapipe,\n                          imageFile,\n                          configFile,\n                          dfm=dfm)\n</code></pre>"},{"location":"api/face-warp/#converting-deformation-files","title":"Converting deformation files","text":"<p>Other face deformation tools developed by our team use the <code>.dfm</code> deformation file format, more suited to applying the same deformation to an arbitrary face. However, by its use of barycentric coordinates in a landmarks triangulation, it isn't suited to any post or pre-processing, which is an area where <code>.dfmxy</code> shines. As a result, CLEESE's Mediapipe provides a way to convert a given, <code>.dfmxy</code> to <code>.dfm</code>, provided you also have the original landmarks on hand:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ndfmxyFile = 'path_to_dfmxy.dfmxy'\ndfmFile = 'path_to_dfm.dfm'\nlandmarksFile = 'path_to_landmarks.txt'\n\nimg = cleese.dfmxy_to_dfm(dfmxyFile,\n                          landmarksFile,\n                          output_dfm_file=dfmFile)\n</code></pre>"},{"location":"api/face-warp/#configuration","title":"Configuration","text":"<p>The following parameters are used to configure the <code>Mediapipe</code> engine:</p> <pre><code>[mediapipe.random_gen]\n# Indices of the landmarks to be modified, using Dlib's 68 landmarks indexing\nlandmarks.dlib = []\n\n# Indices of the landmarks to be modified, using Mediapipe's 468 landmarks indexing\nlandmarks.mediapipe = [61, 40, 78, 91, 270, 308, 321, 291]  # lips corners\n\n# Sets of landmarks to be modified, using precomputed sets\n# \"dlib-eyebrow-right\", \"dlib-eyebrow-left\", \"dlib-nose\",\n# \"dlib-eye-right\", \"dlib-eye-left\", \"dlib-outer-lips\",\n# \"dlib-inner-lips\", \"dlib-lips\", etc...\n# See cleese/engines/mediapipe.py for a full list\nlandmarks.presets = [\"dlib-lips\"]\n\n# Covariance matrix used to generate the gaussian distribution of landmarks\n# offsets. It is scaled according to the height of the detected face. As a\n# result, the amount of deformation should be resolution-invariant.\ncovMat = [[0.0002, 0.0], [0.0, 0.0002]]\n\n[mediapipe.mls]\n# Alpha parameter of the MLS deformation.\n# Affects how much the deformation \"spreads\" from the landmarks\nalpha = 1.2\n\n[mediapipe.face_detect]\n# Minimum face detection confidence\nthreshold = 0.5\n</code></pre> <ul> <li><code>mediapipe.random_gen.landmarks</code>: Selection of landmarks on which to apply a deformation. Then can be defined in a few different ways:<ul> <li><code>landmarks.dlib</code>: Array of landmark indices, using dlib's Multi-PIE 68 landmarks indexing (see figure below).</li> <li><code>landmarks.mediapipe</code>: Array of landmark indices, using mediapipe's 468 vertices face mesh (see Google's documentation here)</li> </ul> </li> <li><code>landmarks.presets</code>: Array of name of landmarks presets. For now, only subset of dlib's indices are implemented.</li> <li><code>covMat</code>: The covariance matrix used when drawing the distribution of deformation vectors. the unit vector is scaled according to height of the detected face, to enable for scale-invariant deformations.</li> <li><code>mediapipe.mls.alpha</code>: Moving Least Squares (MLS) algorithm alpha parameter, affecting the \"spread\" of the deformation.</li> <li><code>mediapipe.face_detect.threshold</code>: Threshold for the confidence metric of mediapipe's face detector. Ajust if faces aren't detected, or if things that aren't faces are detected.</li> </ul> Dlib's 68 Multi-PIE landmarks, 1-indexed."},{"location":"api/general/","title":"General API","text":""},{"location":"api/general/#common-configuration","title":"Common configuration","text":"<p>In order to configure the different tools it provides, CLEESE uses <code>toml</code> configuration files. Most sections in these file are specific to certain engines, but here are a few variables shared among all engines:</p> <pre><code>\\begin{minted}[bgcolor=bg]{toml}\n[main]\n\n# output root folder\noutPath = \"./CLEESE_output_data/\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = true\n</code></pre> <p>Enabling the <code>generateExpFolder</code> option will generate a new folder inside <code>outPath</code> for each subsequent experiment. Whereas if this option is disabled, all experiment results are written directly in <code>outPath</code>.</p>"},{"location":"api/phase-vocoder/","title":"PhaseVocoder Engine","text":"<p>CLEESE's <code>PhaseVocoder</code> engine operates by generating a set of random breakpoint functions (BPFs) in the appropriate format for each treatment, which are then passed to the included spectral processing engine (based on an implementation of the Phase Vocoder algorithm) with the corresponding parameters. Alternatively, the BPFs can be externally created by the user, and so it can also be used as a Phase Vocoder-based effects unit.</p>"},{"location":"api/phase-vocoder/#modes-of-operation","title":"Modes of operation","text":"<p>CLEESE can be used in several different modes, depending on how the main processing function is called. Examples of several typical usage scenarios are included in the example script \\texttt{run_cleese.py}.</p>"},{"location":"api/phase-vocoder/#batch-generation","title":"Batch generation","text":"<p>In batch mode, CLEESE's <code>PhaseVocoder</code> engine generates many random modifications from a single input sound file, called the base sound. It can be launched as follows:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile)\n</code></pre> <p>Two parameters have to be set by the user:</p> <ul> <li> <p><code>inputFile</code>: the path to the base sound, which has to be a mono sound in WAV format.</p> </li> <li> <p><code>configFile</code>: the path to the configuration file</p> </li> </ul> <p>All the generation parameters for all treatments are set up in the configuration file that has to be edited or created by the user. An example of configuration file with parameters for all treatments is included with the toolbox: <code>cleese-phase-vocoder.toml</code>. Configuration parameters will be detailed below.</p> <p>For each run in batch mode, the toolbox generates the following folder structure, where <code>&lt;outPath&gt;</code>} is specified in the parameter file:</p> <ul> <li><code>&lt;outPath&gt;/&lt;currentExperimentFolder&gt;</code>: main folder for the current generation experiment. The name <code>&lt;currentExperimentFolder&gt;</code> iscautomatically created from the current date and time. This folder contains:<ul> <li><code>&lt;baseSound&gt;.wav</code>: a copy of the base sound used for the current experiment</li> <li><code>*.toml</code>: a copy of the configuration script used for the current experiment</li> <li>one subfolder for each one of the performed treatments, which can be either <code>pitch</code>,  <code>stretch</code>, <code>gain</code>, <code>eq</code>, or a combination (chaining) of them. Each of them contains, for each generated stimulus:<ul> <li><code>&lt;baseSound&gt;.xxxxxxxx.&lt;treatment&gt;.wav</code>: the generated stimulus, where <code>xxxxxxxx</code> is a running number (e.g.: <code>cage.00000001.stretch.wav</code>)</li> <li><code>&lt;baseSound&gt;.xxxxxxxx.&lt;treatment&gt;BPF.txt</code>: the generated BPF, in ASCII format, for the generated stimulus (e.g.: <code>cage.00000001.stretchBPF.txt</code>)</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/phase-vocoder/#passing-a-given-bpf","title":"Passing a given BPF","text":"<p>When passing the <code>BPF</code> argument to <code>cleese.generate_stimuli</code>, it is possible to impose a given BPF with a certain treatment to an input file. In this way, the toolbox can be used as a traditional effects unit.</p> <pre><code>import numpy as np\n\nimport cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ngivenBPF = np.array([[0.,0.],[3.,500.]])\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile, BPF=givenBPF)\n</code></pre> <p>The <code>BPF</code> argument can be either:  - a numpy array containing the BPF  - a scalar, in which case the treatment performed is static</p> <p>In this usage scenario, only one file is output, stored at the <code>&lt;outPath&gt;</code> folder, as specified in the configuration file.</p>"},{"location":"api/phase-vocoder/#passing-a-given-time-vector","title":"Passing a given time vector","text":"<p>Instead of passing a full BPF (time+values), it is also possible to just pass a given time vector, containing the time instants (in seconds), at which the treatments change. The amount of modification will be randomly generated, but they will always happen at the given time tags. This might be useful to perform random modifications at specific onset locations, previously obtained manually or automatically.</p> <p>The time vector is passed via the <code>timeVec</code> argument:</p> <pre><code>import numpy as np\n\nimport cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ngivenTimeVec = np.array([0.1,0.15,0.3])\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile, timeVec=givenTimeVec)\n</code></pre>"},{"location":"api/phase-vocoder/#array-input-and-output","title":"Array input and output","text":"<p>Instead of providing a file name for the input sound, it is possible to pass a Numpy array containing the input waveform. In this case, the main function will provide as output both the modified sound and the generated BPF as Numpy arrays. No files or folder structures are created as output:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\nwaveIn,sr,__ = PhaseVocoder.wavRead(inputFile)\nwaveOut,BPFout = cleese.process_data(PhaseVocoder,\n                                     waveIn,\n                                     configFile,\n                                     sample_rate=sr)\n</code></pre> <p>Note that the sampling rate <code>sample_rate</code> has to be passed as well! Like when passing a BPF, only a single sound is generated.</p>"},{"location":"api/phase-vocoder/#configuration-file","title":"Configuration file","text":"<p>All the generation parameters are set in the configuration file. Please refer to the included configuration script <code>cleese-phase-vocoder.toml</code> for an example.</p>"},{"location":"api/phase-vocoder/#main-parameters","title":"Main parameters","text":"<p>The main parameters are set as follows:</p> <pre><code># apply transformation in series (True) or parallel (False)\nchain = true\n\n# transformations to apply\ntransf = [\"stretch\", \"pitch\", \"eq\", \"gain\"]\n\n[analysis]\n# analysis window length in seconds\n# (not to be confused with the BPF processing window lengths)\nwindow.len = 0.04\n\n# number of hops per analysis window\noversampling = 8\n</code></pre> <p>If <code>chain</code> is set to <code>true</code>, the transformations specified in the <code>transf</code> list will be applied as a chain, in the order implied by the list. For instance, the list <code>['stretch','pitch','eq','gain']</code> will produce the output folders <code>stretch</code>, <code>stretch_pitch</code>, <code>stretch_pitch_eq</code> and <code>stretch_pitch_eq_gain</code>, each one containing an additional step in the process chain.</p> <p>If <code>chain</code> is set to <code>false</code>, the transformations will be in parallel (i.e. all starting from the original sound file), producing the output folders <code>stretch</code>, <code>pitch</code>, <code>eq</code> and <code>gain</code>.</p>"},{"location":"api/phase-vocoder/#common-parameters","title":"Common parameters","text":"<p>The following parameters are shared by all treatments, but can take different values for each of them. </p> <p>In the following, <code>&lt;treatment&gt;</code> has to be replaced by one of the strings in <code>['stretch','pitch','eq','gain']</code>:</p> <pre><code># common treatment parameters\n[&lt;treatment&gt;]\n# BPF window in seconds. If 0 : static transformation\nwindow.len = 0.11\n\n# number of BPF windows. If 0 : static transformation\nwindow.count = 6\n\n# 's': force winlength in seconds,'n': force number of windows (equal length)\nwindow.unit = 'n'\n\n# standard deviation (cents) for each BPF point of the random modification\nstd = 300\n\n# truncate distribution values (factor of std)\ntrunc = 1\n\n# type of breakpoint function:\n#      'ramp': linear interpolation between breakpoints\n#      'square': square BPF, with specified transition times at edges\nBPFtype = 'ramp'\n\n# in seconds: transition time for square BPF\ntrTime = 0.02\n</code></pre> <ul> <li> <p><code>window.len</code>: Length in seconds of the treatment window (i.e., the window used to generate the timestamps in the BPFs). It should be longer than         <code>analysis.window.len</code>. This is only used if <code>window.unit = 's'</code> (see below).     Note: static treatment: if <code>window.length = 0</code>, the treatment is static (i.e. it doesn't change with time - an all-flat BPF).</p> </li> <li> <p><code>window.count</code>: Total number of treatment windows. This is only used if <code>window.unit = 'n'</code> (see below).     Note: static treatment: if <code>window.count = 0</code>, the treatment is static (flat BPF).</p> </li> <li><code>window.unit</code>: Whether to enforce window length in seconds (<code>s</code>) or integer number of windows (<code>n</code>).</li> <li><code>std</code>: Standard deviation of a Gaussian distribution from which the random values at each timestamp of the BPFs will be sampled. The unit of the std is specific to each treatment: <ul> <li>for <code>pitch</code>: cents</li> <li>for <code>eq</code> and <code>gain</code>: amplitude dBs</li> <li>for <code>stretch</code>: stretching factor (~&gt;1: expansion, &lt;1: compression)</li> </ul> </li> <li><code>trunc</code>: Factor of the std above which distribution samples are not allowed. If a sample is greater than +/- <code>std * trunc</code>, a new random value         is sampled at that point.</li> <li><code>BPFtype</code>: Type of BPF. Can be either <code>ramp</code> or <code>square</code></li> <li><code>trTime</code>: For BPFs of type <code>square</code>, length in seconds of the transition phases.</li> </ul>"},{"location":"api/phase-vocoder/#bpfs","title":"BPFs","text":"<p>In CLEESE, sound transformations can be time-varying: the amount of modification (e.g. the pitch shifting or time stretching factors) can dynamically change over the duration of the input sound file. The break-point functions (BPFs) determine how these modifications vary over time. For the <code>pitch</code>, <code>stretch</code> and <code>gain</code> treatments, BPFs are one-dimensional (temporal). For the <code>eq</code> treatment, BPFs are two-dimensional (spectro-temporal).</p> <p>As has been seen, BPFs can be either randomly generated by CLEESE or provided by the user.</p>"},{"location":"api/phase-vocoder/#temporal-bpfs","title":"Temporal BPFs","text":"<p>For the <code>pitch</code>, <code>stretch</code> and <code>gain</code> treatments, BPFs are temporal: they are two-column matrices with rows of the form: <pre><code>time, value\nt0, v0\nt1, v1\n... \ntn, vn\n</code></pre> where <code>time</code> is in seconds, and <code>value</code> is in the same units than the transform's <code>std</code> parameter (ex. cents for <code>pitch</code>).</p> Ramp BPF with window specified in seconds Ramp BPF with window specified in number Square BPF with window specified in seconds Square BPF with window specified in number <p>CLEESE can randomly generate one-dimensional BPFs of two types:</p> <ul> <li> <p>Ramps <code>BPFtype = 'ramp'</code>: the BPF is interpreted as a linearly interpolated function. The result is that the corresponding sound parameter is changed gradually (linearly) between timestamps. Examples are shown above for a treatment window defined in seconds (<code>window.unit = 's'</code>), and for a treatment window defined in terms of window number (<code>window.unit = 'n'</code>). Note that in the first case, the length of the last window depends on the length of the input sound. In the second case, all windows have the same length.</p> </li> <li> <p>Square (<code>BPFtype = 'square'</code>): the BPF is a square wave with sharply sloped transitions, whose length is controlled by <code>trTime</code>. Examples are shown above for a treatment window defined in seconds (<code>window.unit = 's'</code>), and for a treatment window defined in terms of window number (<code>window.unit = 'n'</code>).</p> </li> </ul>"},{"location":"api/phase-vocoder/#spectro-temporal-bpfs","title":"Spectro-temporal BPFs","text":"<p>The <code>eq</code> treatment performs time-varying filtering over a number of determined frequency bands. It thus expects a spectro-temporal (two-dimensional) BPF whose rows are defined as follows:</p> <pre><code>time numberOfBands freq1 value1 freq2 value2 freq3 value3 \nt0, m, f1, v1_0, f2, v2_0, ..., fm, vm_0\nt1, m, f1, v1_1, f2, v2_1, ..., fm, vm_1\n...\ntn, m, f1, v1_n, f2, v2_n, ..., fm, vm_n\n</code></pre> <p>The temporal basis can again be generated as <code>ramp</code> or <code>square</code>. In contrast, in the frequency axis, points are always interpolated linearly. Thus, a spectro-temporal BPF can be interpreted as a time-varying piecewise-linear spectral envelope.</p>"},{"location":"api/phase-vocoder/#treatments","title":"Treatments","text":""},{"location":"api/phase-vocoder/#time-stretching-stretch","title":"Time stretching (<code>stretch</code>)","text":"<p>This treatment stretches or compresses locally the sound file without changing the pitch, according to the current stretching factor (oscillating around 1) at the current timestamp. This is the only treatment that changes the duration of the output compared to the base sound. The used algorithm is a phase vocoder with phase locking based on frame-wise peak picking.</p>"},{"location":"api/phase-vocoder/#pitch-shifting-pitch","title":"Pitch shifting (<code>pitch</code>)","text":"<p>The BPF is used to transpose up and down the pitch of the sound, without changing its duration. The used algorithm is a phase vocoder with phase locking based on frame-wise peak picking, followed by resampling on a window-by-window basis.</p>"},{"location":"api/phase-vocoder/#time-varying-equalization-eq","title":"Time-varying equalization (<code>eq</code>)","text":"<p>This treatment divides the spectrum into a set of frequency bands, and applies random amplitudes to the bands. The definition of band edges is constant, the amplitudes can be time-varying. The corresponding BPF is thus two-dimensional</p> <p>There are two possible ways to define the band division:</p> <ul> <li><code>Linear</code> division into a given number of bands between 0 Hz and Nyquist.</li> <li>Division according to a <code>mel</code> scale into a given number of bands. Note that it it possible to specify any number of filters (less or more than         the traditional 40 filters for mel cepstra.</li> </ul> <p>These settings are defined by the following treatment-specific parameters:</p> <pre><code>[eq]\nscale = 'mel'  # mel, linear\nband.count = 10\n</code></pre>"},{"location":"api/phase-vocoder/#time-varying-gain-gain","title":"Time-varying gain (<code>gain</code>)","text":"<p>For gain or level randomization, the BPF is interpolated and interpreted as an amplitude modulator. Note that the corresponding standard deviation is specified in dBs (base-10 logarithm). If the resulting output exceeds the maximum float amplitude of <code>1.0</code>, the whole output signal is normalized. </p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>CLEESE is a Python toolbox to help the generation of randomized sound and image stimuli for neuroscience research. </p> <p>It provides a number of sound and image transformation algorithms (so-called <code>Engines</code>) able e.g. to create natural-sounding expressive variations around an original speech recording, or expressive variations on a human face. It also provides a config file interface to automatize the call to these algorithms, in order e.g. to easily create thousands of random variants from a single base file, which can serve as stimuli for neuroscience experiments. </p> <p>As of version v2.0.0, CLEESE is composed of two engines: <code>PhaseVocoder</code> and <code>Mediapipe</code>: </p> <ul> <li> <p><code>PhaseVocoder</code> allows one to create random fluctuations around an audio file\u2019s original contour of pitch, loudness, timbre and speed (i.e. roughly   defined, its prosody). One of its foreseen applications is the generation of random voice stimuli for reverse correlation experiments in the vein of Ponsot, Burred, Belin &amp; Aucouturier (2018) Cracking the social code of speech prosody using reverse correlation. PNAS, 115(15), 3972-3977.</p> </li> <li> <p><code>FaceWarp</code> uses mediapipe's Face Mesh API to introduce random or precomputed deformation in the expression of a   visage on an image. This engine was designed to produce batches of deformed faces for reverse correlation experiments in the vein of Jack, Garrod, Yu, Caldara, &amp; Schyns (2012). Facial expressions of emotion are not culturally universal. PNAS, 109(19), 7241-7244.</p> </li> </ul>"},{"location":"about/","title":"About CLEESE","text":"<p>CLEESE is a free, standalone Python module, distributed under an open-source MIT Licence on the FEMTO Neuro team github page. </p> <p>It was originally designed in 2018 by Juan Jos\u00e9 Burred, Emmanuel Ponsot and Jean-Julien Aucouturier at STMS Lab (IRCAM/CNRS/Sorbonne Universit\u00e9, Paris - France), and released on the IRCAM Forum plateform. As of 2021, CLEESE is now developped and maintained by the FEMTO Neuro Team at the FEMTO-ST Institute (CNRS/Universit\u00e9 Bourgogne Franche-Comt\u00e9) in Besan\u00e7on - France, and distributed on the team's github page. </p> <p>CLEESE's development was originally funded by the European Research Council (CREAM 335536, 2014-2019, PI: JJ Aucouturier), and has since then received support from Agence Nationale de la Recherche (ANR SEPIA, AND Sounds4Coma), Fondation pour l'Audition (DASHES) and R\u00e9gion Bourgogne-Franche Comt\u00e9 (ASPECT). </p>"},{"location":"about/#cleese-contributors","title":"CLEESE contributors:","text":"<ul> <li>Juan Jos\u00e9 Burred (original development, Phase Vocoder Engine)  jjburred</li> <li>Emmanuel Ponsot (tool specification) </li> <li>JJ Aucouturier (software architecture)  jjau</li> <li>Lara Kermarec (Face warp engine)  nemirwen</li> <li>Paige Tuttosi (Documentation)  chocobearz</li> </ul>"}]}
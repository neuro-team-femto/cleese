{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>CLEESE is a Python toolbox to help the generation of randomized sound and image stimuli for neuroscience research. </p> <p>It provides a number of sound and image transformation algorithms (so-called <code>Engines</code>) able e.g. to create natural-sounding expressive variations around an original speech recording, or expressive variations on a human face. It also provides a config file interface to automatize the call to these algorithms, in order e.g. to easily create thousands of random variants from a single base file, which can serve as stimuli for neuroscience experiments. </p> <p>As of version v2.0.0, CLEESE is composed of two engines: <code>PhaseVocoder</code> and <code>FaceWrap</code>: </p> <ul> <li> <p><code>PhaseVocoder</code> allows one to create random fluctuations around an audio file\u2019s original contour of pitch, loudness, timbre and speed (i.e. roughly   defined, its prosody). One of its foreseen applications is the generation of random voice stimuli for reverse correlation experiments in the vein of Ponsot, Burred, Belin &amp; Aucouturier (2018) Cracking the social code of speech prosody using reverse correlation. PNAS, 115(15), 3972-3977.</p> </li> <li> <p><code>FaceWarp</code> uses mediapipe's Face Mesh API to introduce random or precomputed deformation in the expression of a   visage on an image. This engine was designed to produce batches of deformed faces for reverse correlation experiments in the vein of Jack, Garrod, Yu, Caldara, &amp; Schyns (2012). Facial expressions of emotion are not culturally universal. PNAS, 109(19), 7241-7244.</p> </li> </ul>"},{"location":"about/","title":"About CLEESE","text":"<p>CLEESE is a free, standalone Python module, distributed under an open-source MIT Licence on the FEMTO Neuro team github page. </p> <p>It was originally designed in 2018 by Juan Jos\u00e9 Burred, Emmanuel Ponsot and Jean-Julien Aucouturier at STMS Lab (IRCAM/CNRS/Sorbonne Universit\u00e9, Paris - France), and released on the IRCAM Forum platform. As of 2021, CLEESE is now developed and maintained by the FEMTO Neuro Team at the FEMTO-ST Institute (CNRS/Universit\u00e9 Bourgogne Franche-Comt\u00e9) in Besan\u00e7on - France, and distributed on the team's github page. </p> <p>CLEESE's development was originally funded by the European Research Council (CREAM 335536, 2014-2019, PI: JJ Aucouturier), and has since then received support from Agence Nationale de la Recherche (ANR SEPIA, AND Sounds4Coma), Fondation pour l'Audition (DASHES) and R\u00e9gion Bourgogne-Franche Comt\u00e9 (ASPECT). </p>"},{"location":"about/#citing-cleese","title":"Citing CLEESE","text":"<p>If you use CLEESE in academic work, please cite it as : </p> <pre><code>Burred, JJ., Ponsot, E., Goupil, L., Liuni, M. &amp; Aucouturier, JJ. (2019).\nCLEESE: An open-source audio-transformation toolbox for data-driven experiments in speech and music cognition. \nPLoS one, 14(4), e0205943.\n</code></pre>"},{"location":"about/#cleese-contributors","title":"CLEESE contributors:","text":"<ul> <li>Juan Jos\u00e9 Burred (original development, Phase Vocoder Engine)  jjburred</li> <li>Emmanuel Ponsot (tool specification) </li> <li>JJ Aucouturier (software architecture)  jjau</li> <li>Lara Kermarec (Face warp engine)  nemirwen</li> <li>Paige Tuttosi (Documentation)  chocobearz</li> </ul>"},{"location":"installation/","title":"Getting started","text":"<p>CLEESE is available on pypi and you can download and install the latest version of CLEESE with the following command:</p> <pre><code>pip install cleese-stim\n</code></pre>"},{"location":"installation/#verify-your-installation","title":"Verify your installation","text":"<p>To verify that you have a working CLEESE installation, run the following cell which should return without error. </p> import cleese<pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n</code></pre>"},{"location":"installation/#development-version","title":"Development version","text":"<p>The latest stable version of CLEESE is the one available via <code>pip install</code> and we recommend you start there. </p> <p>For a quick peep at our latest development features, or if you want to contribute, the CLEESE code base is hosted and maintained on https://github.com/neuro-team-femto/cleese</p>"},{"location":"api/face-warp/","title":"Face Warp","text":"<p>CLEESE's <code>FaceWarp</code> engine works by first identifying a set of landmarks on the visage present in the image. Then a gaussian distribution of deformation vectors is applied to a subset of landmarks, and the Moving Least Squares (MLS) algorithm is used to apply the deformation to the image itself. Alternatively, a precomputed set of deformations can also be provided.</p>"},{"location":"api/face-warp/#modes-of-operation","title":"Modes of operation","text":"<p>CLEESE can be used in different modes, depending on which function you call and how. </p>"},{"location":"api/face-warp/#batch-generation","title":"Batch generation","text":"<p>CLEESE has a dedicated function for batch treatments: <code>cleese.generate_stimuli</code>, which is used, in <code>FaceWarp</code>'s case, to generate a number of randomly deformed visages.</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import FaceWarp\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\ncleese.generate_stimuli(FaceWarp, inputFile, configFile)\n</code></pre> <p>Two parameters have to be set by the user:</p> <ul> <li><code>inputFile</code>: the path to the base image, which can be any image format readable and writeable by <code>PIL</code>.</li> <li><code>configFile</code>: the path to the configuration script</li> </ul> <p>All the generation parameters for all treatments are set up in the configuration script that has to be edited or created by the user. An example of configuration script with parameters for all treatments is included with the toolbox: <code>cleese-mediapipe.toml</code>. Configuration parameters will be detailed below.</p> <p>For each run in batch mode, the toolbox generates the following folder structure, where <code>&lt;outPath&gt;</code> is specified in the parameter file:</p> <ul> <li><code>&lt;outPath&gt;/&lt;currentExperimentFolder&gt;</code>: main folder for the current generation experiment. The name <code>&lt;currentExperimentFolder&gt;</code> is automatically created from the current date and time. This folder contains:<ul> <li><code>&lt;baseImage.ext&gt;</code>: a copy of the base image used for the current experiment</li> <li><code>*.toml</code>: a copy of the configuration script used for the current experiment</li> <li><code>&lt;baseimage&gt;.xxxxxxxx.&lt;ext&gt;</code>: the generated deformed image, where <code>xxxxxxxx</code> is a running number (e.g.: <code>monalisa.00000001.jpg</code>)</li> <li><code>&lt;baseimage&gt;.xxxxxxxx.dfmxy</code>: the generated deformation vectors, in CSV format, for the generated stimulus (e.g.: <code>monalisa.00000001.dfmxy</code>)</li> <li><code>&lt;baseimage&gt;.landmarks.txt</code>: the list of all landmarks positions as detected on the original image, in ASCII format, readable with <code>numpy.loadtxt</code>.</li> </ul> </li> </ul>"},{"location":"api/face-warp/#array-input-and-output","title":"Array input and output","text":"<p>CLEESE can also provide a single result, based on data loaded previously in a script or directly loading a file.</p> <p>From array: </p> <p>Here, you can also use <code>PIL.Image</code>'s <code>np.array(Image.open(\"path/image.jpg\").convert(\"RGB\"))</code> to load the image. The <code>Face Mesh</code> API requires that the image be in RGB format.</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import FaceWarp\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\nimg = FaceWarp.load_file(inputFile)\ndeformedImg = cleese.process_data(FaceWarp, img, configFile)\n</code></pre> <p>From file: <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import FaceWarp\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\ndeformedImg = cleese.process_file(FaceWarp, inputFile, configFile)\n</code></pre></p> <p>In both of those cases, no files or folder structures are generated.</p>"},{"location":"api/face-warp/#applying-a-deformation","title":"Applying a deformation","text":"<p>Both <code>cleese.process_data</code> and <code>cleese.process_file</code> function allow for applying a precomputed set of deformations instead of generating them randomly. CLEESE can accept both <code>.dfmxy</code> deformations (absolute cartesian deformation vectors), and <code>.dfm</code> deformations (deformation vectors mapped onto a triangulation of face landmarks (dlib landmarks indices)).</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import FaceWarp\n\ndfmxyFile = 'path_to_dfmxy.dfmxy'\ndfmFile = 'path_to_dfm.dfm'\nimageFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\n# .dfmxy processing\ndfmxy = FaceWarp.load_dfmxy(dfmxyFile)\nimg = cleese.process_file(FaceWarp,\n                          imageFile,\n                          configFile,\n                          dfmxy=dfmxy)\n\n# .dfm processing\ndfm = FaceWarp.load_dfm(dfmFile)\nimg = cleese.process_file(FaceWarp,\n                          imageFile,\n                          configFile,\n                          dfm=dfm)\n</code></pre>"},{"location":"api/face-warp/#converting-deformation-files","title":"Converting deformation files","text":"<p>Other face deformation tools developed by our team use the <code>.dfm</code> deformation file format, more suited to applying the same deformation to an arbitrary face. However, by its use of barycentric coordinates in a landmarks triangulation, it isn't suited to any post or pre-processing, which is an area where <code>.dfmxy</code> shines. As a result, CLEESE's FaceWarp provides a way to convert a given, <code>.dfmxy</code> to <code>.dfm</code>, provided you also have the original landmarks on hand:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import FaceWarp\n\ndfmxyFile = 'path_to_dfmxy.dfmxy'\ndfmFile = 'path_to_dfm.dfm'\nlandmarksFile = 'path_to_landmarks.txt'\n\nimg = cleese.dfmxy_to_dfm(dfmxyFile,\n                          landmarksFile,\n                          output_dfm_file=dfmFile)\n</code></pre>"},{"location":"api/face-warp/#configuration","title":"Configuration","text":"<p>The following parameters are used to configure the <code>FaceWarp</code> engine:</p> <pre><code>[mediapipe.random_gen]\n# Indices of the landmarks to be modified, using Dlib's 68 landmarks indexing\nlandmarks.dlib = []\n\n# Indices of the landmarks to be modified, using Mediapipe's 468 landmarks indexing\nlandmarks.mediapipe = [61, 40, 78, 91, 270, 308, 321, 291]  # lips corners\n\n# Sets of landmarks to be modified, using precomputed sets\n# \"dlib-eyebrow-right\", \"dlib-eyebrow-left\", \"dlib-nose\",\n# \"dlib-eye-right\", \"dlib-eye-left\", \"dlib-outer-lips\",\n# \"dlib-inner-lips\", \"dlib-lips\", etc...\n# See cleese/engines/mediapipe.py for a full list\nlandmarks.presets = [\"dlib-lips\"]\n\n# Covariance matrix used to generate the gaussian distribution of landmarks\n# offsets. It is scaled according to the height of the detected face. As a\n# result, the amount of deformation should be resolution-invariant.\ncovMat = [[0.0002, 0.0], [0.0, 0.0002]]\n\n[mediapipe.mls]\n# Alpha parameter of the MLS deformation.\n# Affects how much the deformation \"spreads\" from the landmarks\nalpha = 1.2\n\n[mediapipe.face_detect]\n# Minimum face detection confidence\nthreshold = 0.5\n</code></pre> <ul> <li><code>mediapipe.random_gen.landmarks</code>: Selection of landmarks on which to apply a deformation. Then can be defined in a few different ways:<ul> <li><code>landmarks.dlib</code>: Array of landmark indices, using dlib's Multi-PIE 68 landmarks indexing (see figure below).</li> <li><code>landmarks.mediapipe</code>: Array of landmark indices, using mediapipe's 468 vertices face mesh (see Google's documentation here)</li> </ul> </li> <li><code>landmarks.presets</code>: Array of name of landmarks presets. For now, only subset of dlib's indices are implemented.</li> <li><code>covMat</code>: The covariance matrix used when drawing the distribution of deformation vectors. the unit vector is scaled according to height of the detected face, to enable for scale-invariant deformations.</li> <li><code>mediapipe.mls.alpha</code>: Moving Least Squares (MLS) algorithm alpha parameter, affecting the \"spread\" of the deformation.</li> <li><code>mediapipe.face_detect.threshold</code>: Threshold for the confidence metric of mediapipe's face detector. Ajust if faces aren't detected, or if things that aren't faces are detected.</li> </ul> Dlib's 68 Multi-PIE landmarks, 1-indexed."},{"location":"api/general/","title":"General API","text":""},{"location":"api/general/#common-configuration","title":"Common configuration","text":"<p>In order to configure the different tools it provides, CLEESE uses <code>toml</code> configuration files. Most sections in these file are specific to certain engines, but here are a few variables shared among all engines:</p> <pre><code>[main]\n\n# output root folder\noutPath = \"./CLEESE_output_data/\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = true\n\n# parameter file extension (default: '.txt')\nparam_ext = '.txt'\n</code></pre> <p>Enabling the <code>generateExpFolder</code> option will generate a new folder inside <code>outPath</code> for each subsequent experiment. Whereas if this option is disabled, all experiment results are written directly in <code>outPath</code>.</p> <p>CLEESE engines store parameter files alongside each stimulus. By default, the name of these files are the same as the corresponding stimulus, with the <code>.txt</code> extention (ex. for <code>PhaseVocoder</code>, <code>file001.wav</code> and the corresponding <code>file001.txt</code>). This extension (and end of the file name) can be changed with the <code>param_ext</code> option (ex. <code>param_ext = '_bpf.txt</code> will store parameter files as e.g. <code>file001_bpf.txt</code>). </p>"},{"location":"api/phase-vocoder/","title":"PhaseVocoder Engine","text":"<p>CLEESE's <code>PhaseVocoder</code> engine operates by generating a set of random breakpoint functions (BPFs) in the appropriate format for each treatment, which are then passed to the included spectral processing engine (based on an implementation of the Phase Vocoder algorithm) with the corresponding parameters. Alternatively, the BPFs can be externally created by the user, and so it can also be used as a Phase Vocoder-based effects unit.</p>"},{"location":"api/phase-vocoder/#modes-of-operation","title":"Modes of operation","text":"<p>CLEESE can be used in several different modes, depending on how the main processing function is called. Examples of several typical usage scenarios are included in the example script \\texttt{run_cleese.py}.</p>"},{"location":"api/phase-vocoder/#batch-generation","title":"Batch generation","text":"<p>In batch mode, CLEESE's <code>PhaseVocoder</code> engine generates many random modifications from a single input sound file, called the base sound. It can be launched as follows:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile)\n</code></pre> <p>Two parameters have to be set by the user:</p> <ul> <li> <p><code>inputFile</code>: the path to the base sound, which has to be a mono sound in WAV format.</p> </li> <li> <p><code>configFile</code>: the path to the configuration file</p> </li> </ul> <p>All the generation parameters for all treatments are set up in the configuration file that has to be edited or created by the user. An example of configuration file with parameters for all treatments is included with the toolbox: <code>cleese-phase-vocoder.toml</code>. Configuration parameters will be detailed below.</p> <p>For each run in batch mode, the toolbox generates the following folder structure, where <code>&lt;outPath&gt;</code>} is specified in the parameter file:</p> <ul> <li><code>&lt;outPath&gt;/&lt;currentExperimentFolder&gt;</code>: main folder for the current generation experiment. The name <code>&lt;currentExperimentFolder&gt;</code> iscautomatically created from the current date and time. This folder contains:<ul> <li><code>&lt;baseSound&gt;.wav</code>: a copy of the base sound used for the current experiment</li> <li><code>*.toml</code>: a copy of the configuration script used for the current experiment</li> <li>one subfolder for each one of the performed treatments, which can be either <code>pitch</code>,  <code>stretch</code>, <code>gain</code>, <code>eq</code>, or a combination (chaining) of them. Each of them contains, for each generated stimulus:<ul> <li><code>&lt;baseSound&gt;.xxxxxxxx.&lt;treatment&gt;.wav</code>: the generated stimulus, where <code>xxxxxxxx</code> is a running number (e.g.: <code>cage.00000001.stretch.wav</code>)</li> <li><code>&lt;baseSound&gt;.xxxxxxxx.&lt;treatment&gt;BPF.txt</code>: the generated BPF, in ASCII format, for the generated stimulus (e.g.: <code>cage.00000001.stretchBPF.txt</code>)</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/phase-vocoder/#passing-a-given-bpf","title":"Passing a given BPF","text":"<p>When passing the <code>BPF</code> argument to <code>cleese.generate_stimuli</code>, it is possible to impose a given BPF with a certain treatment to an input file. In this way, the toolbox can be used as a traditional effects unit.</p> <pre><code>import numpy as np\n\nimport cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ngivenBPF = np.array([[0.,0.],[3.,500.]])\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile, BPF=givenBPF)\n</code></pre> <p>The <code>BPF</code> argument can be either:  - a numpy array containing the BPF  - a scalar, in which case the treatment performed is static</p> <p>In this usage scenario, only one file is output, stored at the <code>&lt;outPath&gt;</code> folder, as specified in the configuration file.</p>"},{"location":"api/phase-vocoder/#passing-a-given-time-vector","title":"Passing a given time vector","text":"<p>Instead of passing a full BPF (time+values), it is also possible to just pass a given time vector, containing the time instants (in seconds), at which the treatments change. The amount of modification will be randomly generated, but they will always happen at the given time tags. This might be useful to perform random modifications at specific onset locations, previously obtained manually or automatically.</p> <p>The time vector is passed via the <code>timeVec</code> argument:</p> <pre><code>import numpy as np\n\nimport cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ngivenTimeVec = np.array([0.1,0.15,0.3])\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile, timeVec=givenTimeVec)\n</code></pre>"},{"location":"api/phase-vocoder/#array-input-and-output","title":"Array input and output","text":"<p>Instead of providing a file name for the input sound, it is possible to pass a Numpy array containing the input waveform. In this case, the main function will provide as output both the modified sound and the generated BPF as Numpy arrays. No files or folder structures are created as output:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\nwaveIn,sr,__ = PhaseVocoder.wavRead(inputFile)\nwaveOut,BPFout = cleese.process_data(PhaseVocoder,\n                                     waveIn,\n                                     configFile,\n                                     sample_rate=sr)\n</code></pre> <p>Note that the sampling rate <code>sample_rate</code> has to be passed as well! Like when passing a BPF, only a single sound is generated.</p>"},{"location":"api/phase-vocoder/#configuration-file","title":"Configuration file","text":"<p>All the generation parameters are set in the configuration file. Please refer to the included configuration script cleese-phase-vocoder.toml for an example.</p>"},{"location":"api/phase-vocoder/#main-parameters","title":"Main parameters","text":"<p>The main parameters are set as follows:</p> <pre><code># apply transformation in series (True) or parallel (False)\nchain = true\n\n# transformations to apply\ntransf = [\"stretch\", \"pitch\", \"eq\", \"gain\"]\n\n[analysis]\n# analysis window length in seconds\n# (not to be confused with the BPF processing window lengths)\nwindow.len = 0.04\n\n# number of hops per analysis window\noversampling = 8\n</code></pre> <p>If <code>chain</code> is set to <code>true</code>, the transformations specified in the <code>transf</code> list will be applied as a chain, in the order implied by the list. For instance, the list <code>['stretch','pitch','eq','gain']</code> will produce the output folders <code>stretch</code>, <code>stretch_pitch</code>, <code>stretch_pitch_eq</code> and <code>stretch_pitch_eq_gain</code>, each one containing an additional step in the process chain.</p> <p>If <code>chain</code> is set to <code>false</code>, the transformations will be in parallel (i.e. all starting from the original sound file), producing the output folders <code>stretch</code>, <code>pitch</code>, <code>eq</code> and <code>gain</code>.</p>"},{"location":"api/phase-vocoder/#common-parameters","title":"Common parameters","text":"<p>The following parameters are shared by all treatments, but can take different values for each of them. </p> <p>In the following, <code>&lt;treatment&gt;</code> has to be replaced by one of the strings in <code>['stretch','pitch','eq','gain']</code>:</p> <pre><code># common treatment parameters\n[&lt;treatment&gt;]\n# BPF window in seconds. If 0 : static transformation\nwindow.len = 0.11\n\n# number of BPF windows. If 0 : static transformation\nwindow.count = 6\n\n# 's': force winlength in seconds,'n': force number of windows (equal length)\nwindow.unit = 'n'\n\n# standard deviation (cents) for each BPF point of the random modification\nstd = 300\n\n# truncate distribution values (factor of std)\ntrunc = 1\n\n# type of breakpoint function:\n#      'ramp': linear interpolation between breakpoints\n#      'square': square BPF, with specified transition times at edges\nBPFtype = 'ramp'\n\n# in seconds: transition time for square BPF\ntrTime = 0.02\n</code></pre> <ul> <li> <p><code>window.len</code>: Length in seconds of the treatment window (i.e., the window used to generate the timestamps in the BPFs). It should be longer than         <code>analysis.window.len</code>. This is only used if <code>window.unit = 's'</code> (see below).     Note: static treatment: if <code>window.length = 0</code>, the treatment is static (i.e. it doesn't change with time - an all-flat BPF).</p> </li> <li> <p><code>window.count</code>: Total number of treatment windows. This is only used if <code>window.unit = 'n'</code> (see below).     Note: static treatment: if <code>window.count = 0</code>, the treatment is static (flat BPF).</p> </li> <li><code>window.unit</code>: Whether to enforce window length in seconds (<code>s</code>) or integer number of windows (<code>n</code>).</li> <li><code>std</code>: Standard deviation of a Gaussian distribution from which the random values at each timestamp of the BPFs will be sampled. The unit of the std is specific to each treatment: <ul> <li>for <code>pitch</code>: cents</li> <li>for <code>eq</code> and <code>gain</code>: amplitude dBs</li> <li>for <code>stretch</code>: stretching factor (~&gt;1: expansion, &lt;1: compression)</li> </ul> </li> <li><code>trunc</code>: Factor of the std above which distribution samples are not allowed. If a sample is greater than +/- <code>std * trunc</code>, a new random value         is sampled at that point.</li> <li><code>BPFtype</code>: Type of BPF. Can be either <code>ramp</code> or <code>square</code></li> <li><code>trTime</code>: For BPFs of type <code>square</code>, length in seconds of the transition phases.</li> </ul>"},{"location":"api/phase-vocoder/#bpfs","title":"BPFs","text":"<p>In CLEESE, sound transformations can be time-varying: the amount of modification (e.g. the pitch shifting or time stretching factors) can dynamically change over the duration of the input sound file. The break-point functions (BPFs) determine how these modifications vary over time. For the <code>pitch</code>, <code>stretch</code> and <code>gain</code> treatments, BPFs are one-dimensional (temporal). For the <code>eq</code> treatment, BPFs are two-dimensional (spectro-temporal).</p> <p>As has been seen, BPFs can be either randomly generated by CLEESE or provided by the user.</p>"},{"location":"api/phase-vocoder/#temporal-bpfs","title":"Temporal BPFs","text":"<p>For the <code>pitch</code>, <code>stretch</code> and <code>gain</code> treatments, BPFs are temporal: they are two-column matrices with rows of the form: <pre><code>time, value\nt0, v0\nt1, v1\n... \ntn, vn\n</code></pre> where <code>time</code> is in seconds, and <code>value</code> is in the same units than the transform's <code>std</code> parameter (ex. cents for <code>pitch</code>).</p> Ramp BPF with window specified in seconds Ramp BPF with window specified in number Square BPF with window specified in seconds Square BPF with window specified in number <p>CLEESE can randomly generate one-dimensional BPFs of two types:</p> <ul> <li> <p>Ramps <code>BPFtype = 'ramp'</code>: the BPF is interpreted as a linearly interpolated function. The result is that the corresponding sound parameter is changed gradually (linearly) between timestamps. Examples are shown above for a treatment window defined in seconds (<code>window.unit = 's'</code>), and for a treatment window defined in terms of window number (<code>window.unit = 'n'</code>). Note that in the first case, the length of the last window depends on the length of the input sound. In the second case, all windows have the same length.</p> </li> <li> <p>Square (<code>BPFtype = 'square'</code>): the BPF is a square wave with sharply sloped transitions, whose length is controlled by <code>trTime</code>. Examples are shown above for a treatment window defined in seconds (<code>window.unit = 's'</code>), and for a treatment window defined in terms of window number (<code>window.unit = 'n'</code>).</p> </li> </ul>"},{"location":"api/phase-vocoder/#spectro-temporal-bpfs","title":"Spectro-temporal BPFs","text":"<p>The <code>eq</code> treatment performs time-varying filtering over a number of determined frequency bands. It thus expects a spectro-temporal (two-dimensional) BPF whose rows are defined as follows:</p> <pre><code>time numberOfBands freq1 value1 freq2 value2 freq3 value3 \nt0, m, f1, v1_0, f2, v2_0, ..., fm, vm_0\nt1, m, f1, v1_1, f2, v2_1, ..., fm, vm_1\n...\ntn, m, f1, v1_n, f2, v2_n, ..., fm, vm_n\n</code></pre> <p>The temporal basis can again be generated as <code>ramp</code> or <code>square</code>. In contrast, in the frequency axis, points are always interpolated linearly. Thus, a spectro-temporal BPF can be interpreted as a time-varying piecewise-linear spectral envelope.</p>"},{"location":"api/phase-vocoder/#treatments","title":"Treatments","text":""},{"location":"api/phase-vocoder/#time-stretching-stretch","title":"Time stretching (<code>stretch</code>)","text":"<p>This treatment stretches or compresses locally the sound file without changing the pitch, according to the current stretching factor (oscillating around 1) at the current timestamp. This is the only treatment that changes the duration of the output compared to the base sound. The used algorithm is a phase vocoder with phase locking based on frame-wise peak picking.</p>"},{"location":"api/phase-vocoder/#pitch-shifting-pitch","title":"Pitch shifting (<code>pitch</code>)","text":"<p>The BPF is used to transpose up and down the pitch of the sound, without changing its duration. The used algorithm is a phase vocoder with phase locking based on frame-wise peak picking, followed by resampling on a window-by-window basis.</p>"},{"location":"api/phase-vocoder/#time-varying-equalization-eq","title":"Time-varying equalization (<code>eq</code>)","text":"<p>This treatment divides the spectrum into a set of frequency bands, and applies random amplitudes to the bands. The definition of band edges is constant, the amplitudes can be time-varying. The corresponding BPF is thus two-dimensional</p> <p>There are two possible ways to define the band division:</p> <ul> <li><code>Linear</code> division into a given number of bands between 0 Hz and Nyquist.</li> <li>Division according to a <code>mel</code> scale into a given number of bands. Note that it it possible to specify any number of filters (less or more than         the traditional 40 filters for mel cepstra.</li> </ul> <p>These settings are defined by the following treatment-specific parameters:</p> <pre><code>[eq]\nscale = 'mel'  # mel, linear\nband.count = 10\n</code></pre>"},{"location":"api/phase-vocoder/#time-varying-gain-gain","title":"Time-varying gain (<code>gain</code>)","text":"<p>For gain or level randomization, the BPF is interpolated and interpreted as an amplitude modulator. Note that the corresponding standard deviation is specified in dBs (base-10 logarithm). If the resulting output exceeds the maximum float amplitude of <code>1.0</code>, the whole output signal is normalized. </p>"},{"location":"tutorials/face/","title":"Tutorial: random speech generation","text":"<p>This tutorial shows how to use CLEESE's <code>FaceWarp</code> engine to generate a arbitrary number of random facial expressions around an original image of a face.</p> <p> <p></p>"},{"location":"tutorials/face/#preambule","title":"Preambule","text":""},{"location":"tutorials/face/#verify-your-installation","title":"Verify your installation","text":"<p>Before starting, please verify that you have a working CLEESE installation, by running the following cell which you return without error. </p> <p>import cleese<pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import FaceWarp\n</code></pre> Check the installation instructions if needed. </p>"},{"location":"tutorials/face/#useful-files","title":"Useful files","text":"<p>In the following, we'll be a number of files which you'll first need to download and store in your path at the indicated place</p> <ul> <li>monalisa.jpg <code>./pics/monalisa.jpg</code> </li> <li>random_lips.toml <code>./configs/random_lips.toml</code></li> <li>monalisa.random.dfmxy <code>./dfm/monalisa.random.dfmxy</code></li> <li>smile.dfm <code>./dfm/smile.dfm</code></li> </ul>"},{"location":"tutorials/face/#preambule_1","title":"Preambule","text":""},{"location":"tutorials/face/#verify-your-installation_1","title":"Verify your installation\u00b6","text":"<p>Before starting, please verify that you have a working CLEESE installation, by running the following cell which you return without error. </p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import FaceWarp\n</code></pre>"},{"location":"tutorials/face/#basic-image-manipulation-with-cleese","title":"Basic image manipulation with CLEESE","text":""},{"location":"tutorials/face/#random-image-deformation","title":"Random image deformation","text":"<p>The most basic usage scenario of CLEESE's <code>FaceWarp</code> engine is to input a single image (here, a close-up of the Mona Lisa, <code>pics/monalisa.jpg</code>)</p> <p></p> <p>and use CLEESE to apply random tranformation to the expression of the face on the image. For this we use the <code>cleese.process_data</code> function that generate a single input from existing array data. We provide this function with the engine we want to use for this transformation (here: <code>FaceWarp</code>), the data itself, and a configuration file specifying the parameters of the transformation.</p> Random face transformation<pre><code>monalisa_file = \"./pics/monalisa.jpg\"\nconfig_file = \"./configs/random_lips.toml\"\n\nimg = FaceWarp.img_read(monalisa_file)\ndeformed = cleese.process_data(FaceWarp, img, config_file)\nFaceWarp.img_write(deformed, \"./pics/monalisa_transformed.jpg\")\n</code></pre> <p></p>"},{"location":"tutorials/face/#direct-file-loading","title":"Direct file loading Batched transforms","text":"<p>CLEESE can also directly load image files and process them using the  <code>cleese.process_file</code> function, like so:</p> <pre><code>monalisa_file = \"./pics/monalisa.jpg\"\nconfig_file = \"./configs/random_lips.toml\"\n\ndeformed = cleese.process_file(FaceWarp, monalisa_file, config_file)\nFaceWarp.img_write(deformed, \"./pics/monalisa_transformed_2.jpg\")\n</code></pre> <p></p> <p>Instead of generating output files one at a time, CLEESE can be used to generate large numbers of manipulated files, each randomly generated using parameters specified in config files as above. This is achieved by using the <code>cleese.generate_stimuli</code> function. This function does not return the generated images, but directly writes them in the folder specified under <code>[main] outPath</code>, and the number of output files generated is given by <code>[main] numFiles</code>, all of which are found in the configuration file: <pre><code>[main]\n\n# output root folder\noutPath = \"output\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = false\n</code></pre></p> <pre><code>monalisa_file = \"./pics/monalisa.jpg\"\nconfig_file = \"./configs/random_lips.toml\"\n\ndeformed = cleese.generate_stimuli(FaceWarp, monalisa_file, config_file)\n</code></pre> <p>In addition to generating 10 images containing randomly deformed faces in the ouptut directory, CLEESE also writes for each image a <code>.dfmxy</code> file containing the deformation vectors applied to each facial landmarks, as well as a file containing the positions of all the landmarks detected on the original image (<code>.landmarks.txt</code>). Additionally, both the original image and configuration files are copied to the output directory.</p>"},{"location":"tutorials/face/#advanced-use","title":"Advanced use","text":""},{"location":"tutorials/face/#applying-existing-deformation","title":"Applying existing deformation","text":"<p>CLEESE's <code>Mediapipe</code> is also able to apply a given deformation set -- for example loaded from a <code>.dfmxy</code> file -- to an image. This can be useful to gauge the combined results of a reverse correlation experiment.</p> <pre><code>dfmxy_file = \"./dfm/monalisa.random.dfmxy\"\nmonalisa_file = \"./pics/monalisa.jpg\"\nconfig_file = \"./configs/random_lips.toml\"\n\ndfmxy = FaceWarp.load_dfmxy(dfmxy_file)\ndeformed = cleese.process_file(FaceWarp,\n                          monalisa_file,\n                          config_file,\n                          dfmxy=dfmxy)\nFaceWarp.img_write(deformed, \"./pics/monalisa_transformed_3.jpg\")\n</code></pre> <p></p>"},{"location":"tutorials/face/#advanced-use_1","title":"Advanced use","text":""},{"location":"tutorials/face/#converting-dfmxy-to-dfm","title":"Converting <code>.dfmxy</code> to <code>.dfm</code>","text":"<p>Other face deformation tools developed by our team use the <code>.dfm</code> deformation file format, more suited to applying the same deformation to an arbitrary face. However, by its use of barycentric coordinates in a landmarks triangulation, it isn't suited to any post or pre-processing, which is an area where <code>.dfmxy</code> shines. As a result, CLEESE's <code>Mediapipe</code> provides a way to convert a given, <code>.dfmxy</code> to <code>.dfm</code>, provided you also have the original landmarks on hand:</p> <pre><code>dfmxy_file = \"./dfm/monalisa.random.dfmxy\"\nlandmarks_file = \"./dfm/monalisa.landmarks.txt\"\ndfm_file = \"./output/converted.dfm\"\n\nFaceWarp.dfmxy_to_dfm(dfmxy_file,\n                       landmarks_file,\n                       output_dfm_file=dfm_file)\n</code></pre>"},{"location":"tutorials/face/#applying-a-dfm","title":"Applying a <code>.dfm</code>","text":"<p>As another compatibility feature, CLEESE's <code>Mediapipe</code> also allow for applying an existing <code>.dfm</code> file to an arbitrary image:</p> <pre><code>dfm_file = \"./dfm/smile.dfm\"\nmonalisa_file = \"./pics/monalisa.jpg\"\nconfig_file = \"./configs/random_lips.toml\"\n\ndfm = FaceWarp.load_dfm(dfm_file)\ndeformed = cleese.process_file(FaceWarp,\n                          monalisa_file,\n                          config_file,\n                          dfm=dfm)\nFaceWarp.img_write(deformed, \"./pics/monalisa_transformed_4.jpg\")\n</code></pre> <p></p>"},{"location":"tutorials/morphing/","title":"Tutorial: morphing","text":"<p>While CLEESE is primarily designed to randomize stimuli by transforming a base stimulus with randomly generated 'filters', its underlying transformation engines can be used to create 'morphings' between 2 predefined stimuli. This tutorial shows how to use CLEESE's <code>PhaseVocoder</code> engine to create sounds that have parametrically intermediate pitch, intensity and duration contours between a source and a target sound. </p> <p>In more details, the procedure involves, first, estimating the pitch, rms or duration contour of both source and target sounds (using e.g. <code>times,pitch = PhaseVocoder.extract_pitch(wave,sr)</code> ), creating intermediate contours by e.g. interpolating linearly between the source and target values at every time point, and applying this contour as a bpf to the source sound. This is not unlike the procedure to flatten files (where we first measure the original pitch contour, create a bpf that compensates that contour, and apply it again) described here). </p>"},{"location":"tutorials/morphing/#preambule","title":"Preambule","text":""},{"location":"tutorials/morphing/#verify-your-installation","title":"Verify your installation","text":"<p>Before starting, please verify that you have a working CLEESE installation, by running the following cell which should return without error. </p> <p>import cleese<pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n</code></pre> Check the installation instructions if needed. </p>"},{"location":"tutorials/morphing/#useful-imports","title":"Useful imports","text":"<p>The following code imports all the python packages that are needed in the rest of this tutorial (which you can <code>pip install</code> if you don't have them already). </p> extra imports<pre><code>import numpy as np\nfrom scipy import signal \nfrom matplotlib import pyplot as plt\n</code></pre>"},{"location":"tutorials/morphing/#useful-files","title":"Useful files","text":"<p>In the following, we'll be a number of files which you'll first need to download and store in your path at the indicated place</p> <ul> <li>apri_Q <code>./sounds/apri_Q</code> : the recording of a word pronounced as a question</li> <li>apri_I <code>./sounds/apri_I</code> : another recording of the same word, this time pronounced as a statement</li> </ul> <p>Note</p> <p>Both sounds were kindly provided by Dr. James Trujillo (University of Amsterdam), who also provided the use case of this tutorial</p> <ul> <li>random_pitch_profile.toml <code>./configs/random_pitch_profile.toml</code> : a configuration file used to manipulate pitch</li> <li>random_speed_profile.toml <code>./configs/random_speed_profile.toml</code> : a configuration file used to manipulate speed/duration</li> <li>random_rms_profile.toml <code>./configs/random_rms_profile.toml</code> : a configuration file used to manipulate intensity/rms</li> </ul>"},{"location":"tutorials/morphing/#morphing-duration-contours","title":"Morphing duration contours","text":"<p>As a first step, we'll use the <code>PhaseVocoder</code> engine's ability to stretch sounds along arbitrary temporal contours in order to morph the duration of the two words. </p>"},{"location":"tutorials/morphing/#analyse-source-and-target-speed-contours-and-find-the-transformation-needed-to-convert-one-into-the-other","title":"Analyse source and target speed contours, and find the transformation needed to convert one into the other","text":"<p>First, let's load the two sounds using the <code>PhaseVocoder.wav_read</code> utility function. Arbitrarily, let's choose our source file (the one to be transformed) to be the question intonation, and our target file the statement/answer intonation. </p> <pre><code>src_wav, sr, frmt =  PhaseVocoder.wav_read('./sounds/apri_Q.wav')\ntarget_wav, sr, frmt =  PhaseVocoder.wav_read('./sounds/apri_I.wav')\n</code></pre> <p>We then estimate the intensity profile of each sound, in order to visualize how they differ. The <code>PhaseVocoder.extract_rms</code> utility simply extracts root-mean-square intensity on every successive time window (duration <code>win</code> sec., here 20ms), and offers the possibility to threshold the resulting time series to only export values larger than a given intensity (put here to 0.02, but you may want to adapt this value manually to e.g. background noise). </p> <p><pre><code>src_times, src_rms = PhaseVocoder.extract_rms(src_wav, sr, win=.02, thresh=0.02, interpolate=False)\ntarget_times, target_rms = PhaseVocoder.extract_rms(target_wav, sr, win=.02,thresh=0.02, interpolate=False)\n\nplt.figure(figsize=(20,5))\nplt.plot(src_times, src_rms, 'o-', label='src')\nplt.plot(target_times, target_rms, 'o-',label='target')\nplt.legend()\n</code></pre> </p> <p>Our source/question word (blue)   has two distinct syllables (A/PRI), the first being a bit shorter, and the second a bit trailing. In comparison, the target/statement version of the same word (orange)   has a longer first syllable, and a shorter second. Similarly, the gap between the two question-syllables is a little shorter than between the two statement-syllables. Transforming the source into the target would therefore entail progressively stretching the first question-syllable, stretching the pause, and shortening the second segment, until the blue curve matches the morphology of the orange curve.</p> <p>Note</p> <p>Note the two words also differ by a lot more than syllable duration: from the graph above, we see that their intensity profiles differ notably on the first syllable (a louder syllable with a sharper onset for the target). Auditorily, it's also obvious that their pitch differs greatly, with an expected final pitch rise in the question/source file. In this section, we only transform duration, but below we will show how to also (or independently) transform intensity and pitch.  </p> <p>Note</p> <p>Note also the two sounds don't start exactly at the same time in the file (the source sound as its onset around 600ms into the file; the target sound around 680ms), as often happens with naturalistic recordings. While this is not necessarily an issue, we'll also be using stretch to normalize that difference, while we're at it (see below)</p>"},{"location":"tutorials/morphing/#format-this-as-a-cleese-bpf","title":"Format this as a CLEESE bpf","text":"<p>To do all that we will create a BPF (breakpoint function) that lists what stretch factor to be applied at which time point in the file. First, we identify the temporal boundaries of the segments that we want to modify in each file: each syllable, as well as the gap between them. </p> <pre><code># extract temporal bounds for the different segments\ndef extract_bounds(times, values): \n    '''\n    returns the start and end of every section composed of either consecutive nan or consecutive non-nans\n    '''\n    indices= [True]+ list(np.diff(np.isnan(values)))\n    indices[-1] = True\n    return list(times[indices])\n\nsrc_bounds = extract_bounds(src_times, src_rms)\ntarget_bounds = extract_bounds(target_times,target_rms)\n\nplt.figure(figsize=(20,5))\nplt.plot(src_times, src_rms, 'o-', label='src')\nfor bound in src_bounds:\n    plt.plot([bound,bound],[0,np.nanmax(src_rms)],'b:')\nplt.plot(target_times, target_rms, 'o-',label='target')\nfor bound in target_bounds: \n    plt.plot([bound,bound],[0,np.nanmax(target_rms)],':r')\nplt.legend()\n</code></pre> <p></p> <p>Next, we define a bpf where the time points correspond to the boundaries of the segments, and the stretch at each of these segments corresponds to the ratio of duration between the corresponding segments in the source and target sound. </p> <p><pre><code># add t=0 in the bounds, so we also compute the stretch necessary to align the onset of the two sounds\nif src_bounds[0] &gt; 0: \n    src_bounds = np.insert(src_bounds,0,0)\nif target_bounds[0] &gt; 0: \n    target_bounds = np.insert(target_bounds,0,0)\n\n# the stretch transformation is acting at each time where RMS was detected in question_wav\nstretch_bpf_times = src_bounds\n\n# the target stretch at each of these ratio of duration between the corresponding segments in questions and statement bounds\nstretch_bpf_val = [target/src for src, target in zip(np.diff(src_bounds),\n                                                              np.diff(target_bounds))]\n\n# display tentative bpf\nstretch_bpf_val_temp = np.insert(stretch_bpf_val,0,stretch_bpf_val[0])\nplt.step(stretch_bpf_times, stretch_bpf_val_temp, 'k-o', where='pre')\nplt.xlabel('time in file')\nplt.ylabel('BPF')\n</code></pre> </p> <p>The resulting bpf, as should be, spreads around the 1:1 ratio (i.e. no change). It starts at 1.13, which corresponds to the stretching (&gt;1) of the initial silence segment (in effect delaying the onset of the question to match the statement onset); then stretches both the first question syllable and the pause between syllables by what happens to be the same ratio (x1.33); and finally shortens the last question syllable by x.875 (i.e. 87% of its original duration). </p> <p>Warning</p> <p>The code above is brittle, as it assumes that there are the same number of segments in both source and target sounds. It is the case in this example (both start &gt; 0, both have 3 segments: one syllable, one pause and one syllable), but this may well not be the case if you use other pairs of sounds. Adjust manually until you have the same number of segments, for instance by changing the <code>extract_rms</code> threshold (which, if set low enought will only output one segment per sound). It is also possible to manually set segment boundaries, e.g. by identifying them in audacity, and not use semi-automatic rms segmentation as we show here (see example here). </p> <p>To apply this bpf, we simply call <code>cleese.process_data</code> on the source sound, passing it as the <code>bpf=BPF</code> argument, and using a configuration file that tells it it should apply the stretch transformation. </p> <p>However, before we can do that, we need a little hack. Up until at least v2.3.2, CLEESE assumes a slightly different format for bpfs when they are passed directly with the <code>bpf=BPF</code> keyword, compared to when they are generated autonomously by the system. When generated by the system, bpfs can be made as <code>square</code> or <code>ramp</code>, depending on the <code>BPFtype</code> parameter in the config file. When custom bpfs are passed directly however, that parameter is ignored (issue 33), and whatever bpf values are passed are automatically interpolated linearly between time points, as if it was a ramp. So, even though <code>stretch_config_file</code> attempts to set <code>BPFtype='square'</code>, that parameter is ignore, and we have to prepare our custom bpf before passing it to <code>cleese.process_data</code> so that its linear interpolation generates the <code>square</code> bpf that we eventually want the function to use. </p> <pre><code># format bpf so it can be interpolated\n\ntransition_time = 0.01 #10ms\n\n# need duration of source file in seconds\nduration = (src_wav.shape[0])/sr\n\n# insert t=0 at the beginning\nstretch_bpf_times = np.insert(stretch_bpf_times, 0,0)\n\n# every time point is replaced by one slightly earlier, and one slightly later, separated by transition_time\nstretch_bpf_times = np.sort(np.concatenate((\n            np.array(stretch_bpf_times[1:-1])-transition_time/2,\n            np.array(stretch_bpf_times[1:-1])+transition_time/2)))\n\n# check boundary conditions\nif stretch_bpf_times[-1] &gt; duration:\n    stretch_bpf_times = np.delete(stretch_bpf_times, -1)\nif stretch_bpf_times[0] &lt; 0:\n    stretch_bpf_times = np.delete(stretch_bpf_times, 0)\n\n# add t=0 and t=duration\nstretch_bpf_times = np.append(stretch_bpf_times, duration)\nstretch_bpf_times = np.insert(stretch_bpf_times, 0, 0.)\n\n# duplicate all bpf values too \nstretch_bpf_val = np.repeat(stretch_bpf_val,2)\n\n# display new bpf\nplt.plot(stretch_bpf_times, stretch_bpf_val, 'k-o')\nplt.xlabel('time in file')\nplt.ylabel('BPF')\n</code></pre> <p></p> <p>The resulting bpf has the same values as before, but is formatted in such a way that its linear interpolation looks like a <code>square</code> bpf (notice we use <code>plt.plot</code> and not <code>plt.step</code> here, compared to previous visualization). </p>"},{"location":"tutorials/morphing/#apply-the-bpf-to-the-source-file","title":"Apply the bpf to the source file","text":"<p>Now we can finally apply the bpf to the file</p> <p><pre><code># config file\nstretch_config_file = \"./configs/random_speed_profile.toml\"\n\nbpf = np.column_stack((stretch_bpf_times,stretch_bpf_val))\n\n# transform sound\ntransf_wav,transf_bpf = cleese.process_data(PhaseVocoder, src_wav, stretch_config_file, sample_rate=sr, BPF=bpf)\n\n# and save in sound file\nPhaseVocoder.wav_write(transf_wav, './sounds/src_stretch.wav', sr, sample_format=frmt)\n</code></pre> Visualize the result: </p> <p><pre><code>transf_times, transf_rms = PhaseVocoder.extract_rms(transf_wav, sr, win=.02, thresh=0.02, interpolate=False)\n\nplt.figure(figsize=(20,5))\nplt.plot(src_times, src_rms, 'o-', label='src')\nplt.plot(target_times, target_rms, 'o-', label='target')\nplt.plot(transf_times, transf_rms, 'o:',label='transf')\nplt.legend()\n</code></pre> </p> <p>The transformed sound (dotted, green) has the expected temporal morphology: it has been shifted in time and is now aligned with the target sound (orange); its first syllable and the pause between syllables have been stretched, and its second syllable has been shortened, all to match the duration of the target sound's. As already noted above, the stretch transformation has only shifted parts of sounds in time, but hasn't modified their intensity: the transformed first syllable is less intense that the target's, while the transformed second syllable is more intense than the target. Such adjustements of intensity will be done below. </p> <p>The resulting sound   has the same rising-pitch intonation as the source sound, but a subtly different prosody from the original question   with a longer first syllable and a longer pause (/ap'pri/), making it sound perhaps a little more hesitating than the original.  One can compare its duration contour with the target sound, which has a markedly different pitch intonation:   </p>"},{"location":"tutorials/morphing/#create-successive-morphings","title":"Create successive morphings","text":"<p>If one want to make intermediate examples, one only has to modulate the stretch factor by multiplying it by a factor between 0 (0% transformation, identical to source) and 1 (100% transformation, similar to target). More precisely, because stretch factors are centered on 1:1, one should multiply its difference to 1, as <code>1 + factor*(stretch_bpf_val - 1)</code></p> <p>For clarity, let's put everything above in a single function</p> <pre><code>def morph_stretch(src_wav,target_wav,sr,factor):\n\n    # extract source RMS\n    src_times, src_rms = PhaseVocoder.extract_rms(src_wav, sr, win=.02, thresh=0.02, interpolate=False)\n\n    # extract target RMS\n    target_times, target_rms = PhaseVocoder.extract_rms(target_wav, sr, win=.02,thresh=0.02, interpolate=False)\n\n    # extract segment boundaries\n    src_bounds = extract_bounds(src_times, src_rms)\n    target_bounds = extract_bounds(target_times,target_rms)\n    if src_bounds[0] &gt; 0: \n        src_bounds = np.insert(src_bounds,0,0)\n    if target_bounds[0] &gt; 0: \n        target_bounds = np.insert(target_bounds,0,0)\n\n    # compute bpf\n    stretch_bpf_times = src_bounds\n    stretch_bpf_val = [target/src for src, target in zip(np.diff(src_bounds),\n                                                              np.diff(target_bounds))]\n    # reformat bpf for interpolation\n    src_duration = (src_wav.shape[0])/sr\n    stretch_bpf_times = np.sort(np.concatenate((\n            np.array(stretch_bpf_times[1:-1])-transition_time/2,\n            np.array(stretch_bpf_times[1:-1])+transition_time/2)))\n    # add t=0 and t=duration\n    stretch_bpf_times = np.append(stretch_bpf_times, src_duration)\n    stretch_bpf_times = np.insert(stretch_bpf_times, 0, 0.)\n    # duplicate all bpf values too \n    stretch_bpf_val = np.repeat(stretch_bpf_val,2)\n\n\n    # modulate the stretch factor by multiplying its difference to 1 by a factor between 0 and 1\n    factor_val = 1 + factor*(stretch_bpf_val - 1) \n    bpf = np.column_stack((stretch_bpf_times,factor_val))\n\n    # transform sound\n    stretch_config_file = \"./configs/random_speed_profile.toml\"\n    transf_wav,transf_bpf = cleese.process_data(PhaseVocoder, src_wav, stretch_config_file, sample_rate=sr, BPF=bpf)\n\n    return transf_wav\n</code></pre> <p>Then let's call <code>morph_stretch</code> with factors varying between 0 and 1, and plot/listen to the results. </p> <p><pre><code>plt.figure(figsize=(20,5))\n\nsrc_times, src_rms = PhaseVocoder.extract_rms(src_wav, sr, win=.02, thresh=0.02, interpolate=False)\nplt.plot(src_times, src_rms, 'o-', label='src')\n\ntarget_times, target_rms = PhaseVocoder.extract_rms(target_wav, sr, win=.02,thresh=0.02, interpolate=False)\nplt.plot(target_times, target_rms, 'o-', label='target')\n\nfactors = np.arange(0,1.1,0.1)\ncolors = plt.cm.jet(np.linspace(0,1,len(factors)))\n\nfor index,factor in enumerate(factors): # % of morphing, between 0% and 100%\n\n    transf_wav = morph_stretch(src_wav,target_wav,sr,factor)\n\n    # save file\n    PhaseVocoder.wav_write(transf_wav, 'src_stretch_%.1f.wav'%factor, sr, sample_format=frmt)\n\n    # visualize the rms contour of that transformation\n    transf_times, transf_rms = PhaseVocoder.extract_rms(transf_wav, sr, win=.02, thresh=0.02, interpolate=False)\n    plt.plot(transf_times, transf_rms, '-',label='transf_%.1f'%factor, alpha=0.3, color=colors[index])\n\nplt.legend()\n</code></pre> </p> <p>We see successive intermediate morphings between source and target behave as expected. Compare e.g. transformation at 0%   transformation at 50%    and transformation at 100% </p>"},{"location":"tutorials/morphing/#morphing-pitch-contours","title":"Morphing pitch contours","text":"<p>To create intermediate pitch contours between a source and a target sound, we proceed similarly by, first, analysing the pitch contour of each sound, and then construct bpfs that convert one contour into the other. </p> <p>We start again with the same two sounds: </p> <pre><code>src_wav, sr, frmt =  PhaseVocoder.wav_read('./sounds/apri_Q.wav')\ntarget_wav, sr, frmt =  PhaseVocoder.wav_read('./sounds/apri_I.wav')\n</code></pre>"},{"location":"tutorials/morphing/#analyse-source-and-target-pitch-contour-and-compute-the-transformation-needed-to-convert-one-into-the-other","title":"Analyse source and target pitch contour, and compute the transformation needed to convert one into the other","text":"<pre><code>src_times, src_pitch = PhaseVocoder.extract_pitch(src_wav, sr, win=.02, bounds=[140,210], harmo_thresh=0.3, interpolate=True)\ntarget_times, target_pitch = PhaseVocoder.extract_pitch(target_wav, sr, win=.02, bounds=[100,210], harmo_thresh=0.3, interpolate=True)\n\nplt.figure(figsize=(20,5))\nplt.plot(src_times, src_pitch, label='src')\nplt.plot(target_times, target_pitch, label='target')\nplt.legend()\n</code></pre> <p>As can clearly be heard, the source/question sound has a typical rising-pitch intonation, while the target has a descending pitch. Note that the target is higher than the source on the first syllable. So in order to convert one into the other, we will need to increase pitch in the first part of the sound, and lower it in the second part. </p> <p>In order to write such a transformation as a bpf, we need to compare the pitch contour of the source to the pitch contour of the target, at every time point. Because both sounds are not aligned in time (they would be if we used the stretch morphing above first), and do not have the same duration, we can't directly compare them. First, we therefore resample the target pitch contour so that it has the same duration as the source. </p> <pre><code>plt.figure(figsize=(20,5))\nplt.plot(src_times, src_pitch,'o-', label='src')\nplt.plot(target_times, target_pitch, 'o-', label='target')\n\ntarget_pitch_resampled = signal.resample(target_pitch, len(src_pitch))\nplt.plot(src_times, target_pitch_resampled, 'o:', label='target_resampled')\nplt.legend()\n</code></pre> <p></p> <p>We then compute, at each time point in the source file, what is the needed pitch transformation to convert <code>src_pitch</code> into the resampled <code>target_pitch</code>. CLEESE's <code>PhaseVocoder</code> algorithm expects shift values in cents (i.e. 1% of a semitone), which can be obtained from a ratio of frequency (Hz) <code>f_target/f_source</code> as <code>1200*np.log2(f_target/f_source)</code> (double-check: if f_target = 2 x f_source, we go up by one octave, which is 12 tones x 100 cents = 1200 cents, and 1200 x np.log2(2) is indeed +1200). </p> <pre><code>pitch_bpf_times = src_times\n\n# the bpf_value is the cent transformation needed to convert question_pitch to target_pitch\ndef difference_to_cents(src_pitch_val, target_pitch_val):\n    return 1200*np.log2(target_pitch_val/src_pitch_val)\n\n# the target pitch at each of these times is the corresponding pitch in resampled_target_pitch\npitch_bpf_val = np.array([difference_to_cents(src, target) for src, target in zip(src_pitch,target_pitch_resampled)])\n\n# display original file\nplt.plot(1000*pitch_bpf_times, pitch_bpf_val, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('BPF')\nplt.plot([1000*np.min(pitch_bpf_times),1000*np.max(pitch_bpf_times)],[0,0],'k:')\n</code></pre> <p></p> <p>The resulting bpf indeed increases pitch by about one musical tone (+200 cents) at the beginning of the sound, and lowers it dramatically (-700cents, which is down 3.5 musical tones, also called a 'fifth'). </p> <p>Warning</p> <p>Transformations by more than a few semitones (e.g. +/- 300 cents) are usually considered large, and are notoriously difficult to do without altering the timbre/naturalness of the original sound. The <code>PhaseVocoder</code> algorithm currently implemented in CLEESE is a relatively vanilla variant of the classic Phase Vocoder algorithm, and while it includes a number of well-know improvements (e.g. phase-locking), it remains limited in its ability to achieve large shifts of the kind seen here at the end of the sound. The algorithm will do what asked, but the result is likely to be severely distorted (spoiler: indeed, we'll see below). </p>"},{"location":"tutorials/morphing/#reformat-bpf","title":"Reformat bpf","text":"<p>As above, this bpf cannot be directly passed to <code>cleese.process</code> and needs to be reformatted so that it can be linearly interpolated by the engine with changing its morphology. For this, we insert a segment between t=0 and the beginning of the source with a ratio of 0 cents (i.e. no transformation), and we insert a similar segment between the end of the source and t=duration.  </p> <pre><code># format bpf so it can be interpolated\ntransition_time = 0.05 #10ms\nsrc_duration = (src_wav.shape[0])/sr\n\n# insert pitch 0 between t=0 and t=pitch_bpf_times[0]-transition_time/2\npitch_bpf_times = np.insert(pitch_bpf_times,0,pitch_bpf_times[0]-transition_time/2)\npitch_bpf_val = np.insert(pitch_bpf_val,0,0)\npitch_bpf_times = np.insert(pitch_bpf_times,0,0)\npitch_bpf_val = np.insert(pitch_bpf_val,0,0)\n\n# insert pitch 0 between t=pitch_bpf_times[-1]+transition_time/2 and t=duration\npitch_bpf_times = np.append(pitch_bpf_times,pitch_bpf_times[-1]+transition_time/2)\npitch_bpf_val = np.append(pitch_bpf_val,0)\npitch_bpf_times = np.append(pitch_bpf_times,duration)\npitch_bpf_val = np.append(pitch_bpf_val,0)\n\n# display original file\nplt.plot(1000*pitch_bpf_times, pitch_bpf_val, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('BPF')\nplt.plot([1000*np.min(pitch_bpf_times),1000*np.max(pitch_bpf_times)],[0,0],'k:')\n</code></pre> <p></p>"},{"location":"tutorials/morphing/#apply-bpf-to-source-file","title":"Apply bpf to source file","text":"<p>Finally, the bpf can be applied to the source file, using the <code>random_pitch_profile.toml</code> config file. </p> <pre><code>bpf = np.column_stack((pitch_bpf_times,pitch_bpf_val))\n\n# apply transformation\npitch_config_file = \"./configs/random_pitch_profile.toml\"\ntransf_wav,transf_bpf = cleese.process_data(PhaseVocoder, src_wav, pitch_config_file, sample_rate=sr, BPF=bpf)\n</code></pre>"},{"location":"tutorials/morphing/#create-successive-morphings_1","title":"Create successive morphings","text":"<p>To create morphings, one need to modulate the bpf values with a morphing factor between 0 and 1. Contrary to stretch factors (that are multiplicative, hence centered on 1:1), pitch factors are additive and centered on 0, so modulating the bpf just amounts to multiplying its value, i.e. <code>bpf = np.column_stack((pitch_bpf_times,factor*pitch_bpf_val))</code></p> <p>As above, let's first gather all this code into a single <code>morph_pitch</code> function. </p> <pre><code>def morph_pitch(src_wav,target_wav,sr,factor): \n\n    # extract pitch\n    src_times, src_pitch = PhaseVocoder.extract_pitch(src_wav, sr, win=.02, bounds=[140,210], harmo_thresh=0.3, interpolate=True)\n    target_times, target_pitch = PhaseVocoder.extract_pitch(target_wav, sr, win=.02, bounds=[100,210], harmo_thresh=0.3, interpolate=True)\n\n    # resample to adapt to src time point\n    target_pitch_resampled = signal.resample(target_pitch, len(src_pitch))\n\n    # compute bpf\n    pitch_bpf_times = src_times\n    def difference_to_cents(src_pitch_val, target_pitch_val):\n        return 1200*np.log2(target_pitch_val/src_pitch_val)\n    pitch_bpf_val = np.array([difference_to_cents(src, target) for src, target in zip(src_pitch,target_pitch_resampled)])\n\n    # reformat bpf for interpolation\n    src_duration = (src_wav.shape[0])/sr\n    transition_time = 0.05 #10ms\n    # insert pitch 0 between t=0 and t=pitch_bpf_times[0]-transition_time/2\n    pitch_bpf_times = np.insert(pitch_bpf_times,0,pitch_bpf_times[0]-transition_time/2)\n    pitch_bpf_val = np.insert(pitch_bpf_val,0,0)\n    pitch_bpf_times = np.insert(pitch_bpf_times,0,0)\n    pitch_bpf_val = np.insert(pitch_bpf_val,0,0)\n\n    # insert pitch 0 between t=pitch_bpf_times[-1]+transition_time/2 and t=duration\n    pitch_bpf_times = np.append(pitch_bpf_times,pitch_bpf_times[-1]+transition_time/2)\n    pitch_bpf_val = np.append(pitch_bpf_val,0)\n    pitch_bpf_times = np.append(pitch_bpf_times,src_duration)\n    pitch_bpf_val = np.append(pitch_bpf_val,0)\n\n    # modulate the pitch factor by multiplying it by a factor between 0 and 1\n    bpf = np.column_stack((pitch_bpf_times,factor*pitch_bpf_val))\n\n    # apply transformation\n    pitch_config_file = \"docs/docs/tutorials/configs/random_pitch_profile.toml\"\n    transf_wav,transf_bpf = cleese.process_data(PhaseVocoder, src_wav, pitch_config_file, sample_rate=sr, BPF=bpf)\n\n    return transf_wav,transf_bpf\n</code></pre> <p>and we now call it for all factors between 0 and 1</p> <pre><code>plt.figure(figsize=(20,5))\nplt.plot(src_times, src_pitch, 'o-', label='src')\nplt.plot(src_times, target_pitch_resampled, 'o-', label='target')\n\nfactors = np.arange(0,1.1,0.1)\ncolors = plt.cm.jet(np.linspace(0,1,len(factors)))\n\nfor index,factor in enumerate(factors): # % of morphing, between 0% and 100%\n\n    # apply transformation\n    transf_wav, transf_bpf = morph_pitch(src_wav,target_wav,sr,factor)\n\n    # save file\n    PhaseVocoder.wav_write(transf_wav, 'src_pitch_%.1f.wav'%factor, sr, sample_format=frmt)\n\n    # visualize the pitch contour of that transformation\n    transf_times, transf_pitch = PhaseVocoder.extract_pitch(transf_wav, sr, win=.02, bounds=[140,210], harmo_thresh=0.3, interpolate=True)\n    plt.plot(transf_times, transf_pitch, '-',label='transf_%.1f'%factor, alpha=0.3,color=colors[index])\n\nplt.legend()\n</code></pre> <p></p> <p>As seen here, going from the source/question (blue) to the target/statement (orange) creates intermediate pitch contours that more or less interpolates between the two extremes. </p> <p>Note</p> <p>You may wonder why the interpolation seen in the above picture is not perfect, while it was yet created mathematically by an exact multiplication of the value. That's because what we're seeing here is not the bpf, but an algorithmic evaluation (<code>extract_pitch</code>) of the pitch of the resulting sound. While the bpf is exactly interpolated, the pitch transformation algorithm may not perfectly recreate the target pitch (we'll see below that it is the case for large transformations) and the pitch analysis algorithm may not perfectly recognize the pitch in the transformed sound (that's notably likely when sounds are distorted by the transformation). </p> <p>Here's an audio montage of the successive intermediate pitch morphings between source and target obtained with this procedure   One can hear that the initial 3 or 4 steps are relatively useable, but that larger pitch transformations above step 5 or so severely distorts the timbre of the sound, making e.g. the second part of sound increasingly deep and masculine, to the point of sounding completely irrealistic. This illustrates the limitation of the relatively simple <code>PhaseVocoder</code> implementation in CLEESE, which is only really useful for small pitch transformations in the +/-300 cent range. </p> <p>One possibility to circumvent this limitation is, of course, to generate morphings in the opposite direction (from target to source), and \"meet in the middle\". </p> <p><pre><code>plt.figure(figsize=(20,5))\nplt.plot(target_times, target_pitch, 'o-', label='src')\n\nsrc_pitch_resampled = signal.resample(src_pitch, len(target_pitch))\nplt.plot(target_times, src_pitch_resampled, 'o-', label='target')\n\nfactors = np.arange(0,1.1,0.1)\ncolors = plt.cm.jet(np.linspace(0,1,len(factors)))\n\nfor index,factor in enumerate(factors): # % of morphing, between 0% and 100%\n\n    # apply transformation\n    transf_wav, transf_bpf = morph_pitch(target_wav,src_wav,sr,factor)\n\n    # save file\n    PhaseVocoder.wav_write(transf_wav, 'target_pitch_%.1f.wav'%factor, sr, sample_format=frmt)\n\n    # visualize the pitch contour of that transformation\n    transf_times, transf_pitch = PhaseVocoder.extract_pitch(transf_wav, sr, win=.02, bounds=[140,210], harmo_thresh=0.3, interpolate=True)\n    plt.plot(transf_times, transf_pitch, '-',label='transf_%.1f'%factor, alpha=0.3,color=colors[index])\n\nplt.legend()\n</code></pre> </p> <p>As before, the transformation from statement to question is relatively useable for the first 3-4 steps, after which the voice takes on an unnatural \"chipmunk\" quality.    Combining the two (first few steps of target to source, followed by first few steps of source to target in reverse order) creates a relatively convincing interpolation of pitch contours (although one of course can clearly hear the bifurcation of speed contour halfway in the sequence - for this we need to combine stretch and pitch morphing as seen below). </p>"},{"location":"tutorials/morphing/#morphing-rms","title":"Morphing RMS","text":"<p>Morphing the rms contours between 2 sounds works in the same manner as above. We simply provide here the combined function. </p> <pre><code>def morph_rms(src_wav,target_wav,sr,factor): \n\n    # extract rms with no threshold\n    src_times, src_rms = PhaseVocoder.extract_rms(src_wav, sr, win=.02, thresh=0, interpolate=False)\n    target_times, target_rms = PhaseVocoder.extract_rms(target_wav, sr, win=.02,thresh=0, interpolate=False)\n\n    # resample to align time\n    target_rms_resampled = signal.resample(target_rms, len(src_rms))\n    target_rms_resampled = np.where(target_rms_resampled&lt;0, 0, target_rms_resampled)\n\n    # compute bpf\n    rms_bpf_times = src_times\n    rms_bpf_val = np.array([target/src for src, target in zip(src_rms,target_rms_resampled)])\n    rms_bpf_val = np.where(src_rms&lt;0.01, 1, rms_bpf_val) # threshold/simplify the bpf when the original sound is silence. \n\n    # modulate the stretch factor by multiplying its difference to 1 by a factor between 0 and 1\n    factor_val = 1 + factor*(rms_bpf_val - 1) \n    bpf = np.column_stack((rms_bpf_times,factor_val))\n\n    # apply transformation\n    rms_config_file = \"./configs/random_rms_profile.toml\"\n    transf_wav,transf_bpf = cleese.process_data(PhaseVocoder, src_wav, rms_config_file, sample_rate=sr, BPF=bpf)\n\n    # normalize rms\n    transf_wav = transf_wav/np.max(np.abs(transf_wav))*0.999 \n\n    return transf_wav,transf_bpf\n</code></pre> <pre><code>plt.figure(figsize=(20,5))\nplt.plot(src_times, src_rms, 'o-', label='src')\nplt.plot(src_times, target_rms_resampled, 'o-', label='target')\n\nfactors = np.arange(0,1.1,0.1)\ncolors = plt.cm.jet(np.linspace(0,1,len(factors)))\n\nfor index,factor in enumerate(factors):  # % of morphing, between 0% and 100%\n\n    # apply transformation\n    transf_wav,transf_bpf = morph_rms(src_wav,target_wav,sr,factor)\n\n    # save file\n    PhaseVocoder.wav_write(transf_wav, 'src_rms_%.1f.wav'%factor, sr, sample_format=frmt)\n\n    # visualize the rms contour of that transformation\n    transf_times, transf_rms = PhaseVocoder.extract_rms(transf_wav, sr, win=.02,thresh=0, interpolate=False)\n    plt.plot(transf_times, transf_rms, '-',label='transf_%.1f'%factor, alpha=0.3, color=colors[index])\n\nplt.legend()\n</code></pre> <p></p>"},{"location":"tutorials/morphing/#combining-transformations","title":"Combining transformations","text":"<p>It is of course possible to combine/chain these transformations. For instance, one may want to create morphings that progressively modifies both the pitch and the speed contour of the sounds. </p> <pre><code>factors = np.arange(0,1.1,0.1)\ncolors = plt.cm.jet(np.linspace(0,1,len(factors)))\n\nfor index,factor in enumerate(factors): # % of morphing, between 0% and 100%\n\n    transf1_wav, transf_bpf = morph_stretch(src_wav,target_wav,sr,factor)    \n    transf2_wav, transf_bpf = morph_pitch(transf1_wav,target_wav,sr,factor)    \n    PhaseVocoder.wav_write(transf2_wav, 'src_to_target_all_%.1f.wav'%factor, sr, sample_format=frmt)\n\n    transf1_wav, transf_bpf = morph_stretch(target_wav,src_wav,sr,factor)    \n    transf2_wav, transf_bpf = morph_pitch(transf1_wav,src_wav,sr,factor)    \n    PhaseVocoder.wav_write(transf2_wav, 'target_to_src_all_%.1f.wav'%factor, sr, sample_format=frmt)\n</code></pre> <p>For instance, here are the first 4 steps of moving from a statement to a question. </p>"},{"location":"tutorials/speech/","title":"Tutorial: random speech generation","text":"<p>This tutorial shows how to use CLEESE's <code>PhaseVocoder</code> engine to generate a arbitrary number of expressive variations around an original speech recording.</p>"},{"location":"tutorials/speech/#6-random-variants-of-french-phase-je-suis-en-route-pour-la-reunion-im-on-my-way-to-the-meeting","title":"6 random variants of French phase \"je suis en route pour la r\u00e9union\" (I'm on my way to the meeting)","text":"<p>  Download audio </p>"},{"location":"tutorials/speech/#preambule","title":"Preambule","text":""},{"location":"tutorials/speech/#verify-your-installation","title":"Verify your installation","text":"<p>Before starting, please verify that you have a working CLEESE installation, by running the following cell which you return without error. </p> <p>import cleese<pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n</code></pre> Check the installation instructions if needed. </p>"},{"location":"tutorials/speech/#useful-imports","title":"Useful imports","text":"<p>The following code imports all the python packages that are needed in the rest of this tutorial (which you can <code>pip install</code> if you don't have them already). </p> extra imports<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"tutorials/speech/#useful-files","title":"Useful files","text":"<p>In the following, we'll be a number of files which you'll first need to download and store in your path at the indicated place</p> <ul> <li>male_vraiment_flattened.wav <code>./sounds/male_vraiment_flattened.wav</code> </li> <li>random_pitch_profile.toml <code>./configs/random_pitch_profile.toml</code></li> <li>female_anniversaire_isochrone.wav <code>./sounds/female_anniversaire_isochrone.wav</code></li> <li>random_speed_profile.toml <code>./configs/random_speed_profile.toml</code></li> <li>chained_pitch_stretch.toml :material-arrow-right <code>./configs/chained_pitch_stretch.toml</code></li> <li>male_vraiment_original.wav <code>./sounds/male_vraiment_original.wav</code> </li> </ul>"},{"location":"tutorials/speech/#basic-sound-manipulation-with-cleese","title":"Basic sound manipulation with CLEESE","text":""},{"location":"tutorials/speech/#random-pitch-profile-in-a-single-utterance","title":"Random pitch profile in a single utterance","text":"<p>The most basic usage scenario of CLEESE is to input a single recording (ex. the French word \"vraiment\" - \"really\", recorded by a single male speaker)</p> <p>  Download audio </p> <p>and use CLEESE to transform the sound with a random pitch profile. This, like all cleese operations, is done by passing to the main cleese function <code>cleese.process_data</code> a configuration file (random_pitch_profile.toml) which specifies the manipulation we want. Here: cut the file in <code>pitch.window.count = 6</code> time segments, draw a random pitch shift factor at each segment boundary from a Gaussian distribution centered on 0 and standard deviation <code>pitch.std = 300</code>cents, and interpolate between segment boundaries using linear <code>pitch.BPFType = \"ramp\"</code>. (See PhaseVocoder documentation for more information)</p> random_pitch_profile.toml<pre><code>[pitch]\n# pitch transposition window in seconds. If 0 : static transformation\nwindow.len = 0.11\n\n# number of pitch transposition windows. If 0 : static transformation\nwindow.count = 6\n\n# 's': force winlength in seconds,'n': force number of windows (equal length)\nwindow.unit = 'n'\n\n# standard deviation (cents) for random transposisiton (Gaussian distrib for now)\nstd = 300\n\n# truncate distribution values (factor of std)\ntrunc = 1\n\n# type of breakpoint function:\n#      'ramp': linear interpolation between breakpoints\n#      'square': square BPF, with specified transition times at edges\nBPFtype = 'ramp'\n\n# in s: transition time for square BPF\ntrTime = 0.02\n</code></pre> <p>The following code is pretty much all there is to call: <code>cleese.process_data</code> takes in the <code>Engine</code> that is called on to do the transformation (here, <code>PhaseVocoder</code> - see the Image tutorial for a similar call to image-transformation engine <code>FaceWarp</code>), the array <code>wave_in</code> of audio data obtained from <code>PhaseVocoder.wav_read</code> and its sampling rate <code>sr</code>, and the path to the configuration file <code>config_file</code> that tells the <code>PhaseVocoder</code> engine what to do with it all. </p> randomize pitch<pre><code>input_file = \"./sounds/male_vraiment_flattened.wav\"\nconfig_file = \"./configs/random_pitch_profile.toml\"\n\n# read input wavefile\nwave_in, sr, _ = PhaseVocoder.wav_read(input_file)\n\n# transform sound\nwave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr)\n\n# save file if necessary\noutput_file = \"./sounds/male_vraiment_flattened_transformed.wav\"\nPhaseVocoder.wav_write(wave_out, output_file, sr)\n</code></pre> <p>  Download audio </p> <p>The <code>bpf_out</code> output describes the break-point function that was generated randomly by CLEESE and used to transform <code>wave_in</code> to <code>wave_out</code>. It is a simple array of times and values pairs, each corresponding to the timepoint in the file of a breakpoint and the corresponding pitch transformation values applied at this point (for more on BPFs, see the PhaseVocoder documentation).  </p> <p><pre><code>print(bpf_out)\n</code></pre> <pre><code>[[ 0.00000000e+00 -1.01767821e+02]\n [ 7.10430839e-02 -9.91175403e+01]\n [ 1.42086168e-01  1.07735422e+02]\n [ 2.13129252e-01 -3.87725633e+01]\n [ 2.84172336e-01  5.15893896e+01]\n [ 3.55215420e-01  1.46774252e+02]\n [ 4.26258503e-01 -5.06624397e+00]]\n</code></pre> Here, one sees that the file duration is about 426ms and that, consistently with the <code>window.count = 6</code> parameter in the config file, the transformation uses 7 breakpoints, i.e. 6 segments. Each breakpoint is associated a pitch transformation value in cents which, here, was assigned random samples ranging between -101.7 and +146.7, which is consistent with the <code>std = 300</code> parameter in the config_file. The general shape of the transformation, in that specific random instance, is to reduce the beginning of the sound by ca. 100 cents (i.e. - 1 semitone), then increase it by +100 cents (compared to baseline) around 142ms, down again at 213ms, back up to +146cents at 355ms, i.e. a shape that is roughly that of letter W. Of course, every call to the same code will generate a new, random bpf and transformation. </p> <p>About cents</p> <p>Cents are a relative unit of frequency which corresponds to 1% of a musical semitone. Increasing a frequency \\(f_1\\) by \\(+n\\) cents results in \\(f_2 = f_1 * 2^{n/1200}\\). Increasing sound's pitch by +100cents is equivalent to raising it by 1 semitone (\\(f_1*2^{1/12})\\), i.e. the same as going from musical notes C to C#. A change of 12 semitone (1200 cents) corresponds to going up one octave (e.g. C3  C4), corresponding to doubling frequency (\\(f_1 * 2^{1200/1200}\\)). In speech, pitch changes of the order of 100-200 cents are considered large; a static change of +50cents is often sufficient to evoke e.g. the impression of happier speech (Rachman et al. 2018). Technically, CLEESE's <code>PhaseVocoder</code> engine implements a relatively straightforward version of the phase vocoder algorithm (phase locking with frame-wise peak picking and no spectral envelope conservation, see e.g. Laroche and Dolson, 1999), and will likely generate artifacts such as phasiness for transformations larger than 150-200 cents (which may or may not be a problem depending on your usage scenario) </p> <p>CLEESE's <code>PhaseVocoder</code> includes a utility for extracting pitch in speech/audio files (<code>PhaseVocoder.extract_pitch</code>), which uses the YIN pitch extraction algorithm, and can be used to visualize the pitch profile of sounds before and after manipulation. This is just for visualization purposes, and isn't necessary for the working of the main <code>cleese.process</code> function above. </p> visualize pitch before/after<pre><code># extract pitch before transformation\ntimes_in,pitch_in = PhaseVocoder.extract_pitch(wave_in,sr)\n\n# extract pitch after transformation\ntimes_out,pitch_out = PhaseVocoder.extract_pitch(wave_out,sr)\n\n# display \nplt.plot(times_in, pitch_in, 'k:', label='pre')\nplt.plot(times_out, pitch_out, 'k', label='post')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch (Hz)')\nplt.ylim([70,120])\n</code></pre> <p></p>"},{"location":"tutorials/speech/#random-speed-profile-in-a-song","title":"Random speed profile in a song","text":"<p>CLEESE can process longer files than a single word and, instead of manipulating pitch, can manipulate the duration of each portion of the file. To demonstrate this, we use CLEESE to randomly stretch each note in a recording of a song (the French song \"Joyeux Anniversaire\" / \"Happy Birthday\", sung by a female singer)</p> <p>  Download audio </p> <p>This, as above, is done by passing to <code>cleese.process_data</code> a configuration file which specifies the manipulation we want. Here: cut the file in <code>stretch.window.len = 0.5</code> second time segments, draw a random stretch shift factor at each segment boundary from a Gaussian distribution centered on 1.0 and standard deviation <code>stretch.std = 1.5</code> (where factors &gt;1 correspond to a time stretch, and factors &lt;1 correspond to a time compression), and interpolate between segment boundaries using linear <code>stretch.BPFType = \"ramp\"</code>. </p> random_speed_profile.toml<pre><code>[stretch]\n\nwindow.len = 0.1\nwindow.count = 5\nwindow.unit = 'n'\n\n# stretching factor. &gt;1: expansion, &lt;1: compression\nstd = 1.5\ntrunc = 1\nBPFtype = 'ramp'\ntrTime = 0.05\n</code></pre> <p>The following code runs the transformation</p> randomize duration<pre><code>input_file = \"./sounds/female_anniversaire_isochrone.wav\"\nconfig_file = \"./configs/random_speed_profile.toml\"\n\n# read input wavefile\nwave_in, sr, _ = PhaseVocoder.wav_read(input_file)\n\n# CLEESE\nwave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr)\n\n# save file if necessary\noutput_file = \"./sounds/female_anniversaire_isochrone_transformed.wav\"\nPhaseVocoder.wav_write(wave_out, output_file, sr)\n</code></pre> <p>  Download audio </p> <p><pre><code>print(bpf_out)\n</code></pre> <pre><code>[[0.         0.72650135]\n [0.64062585 1.        ]\n [1.2812517  1.42662854]\n [1.92187755 1.16536151]\n [2.5625034  2.382554  ]\n [3.20312925 1.28544142]]\n</code></pre></p> <p>Again, inspection of the (randomly generated) BPF shows 5 segments/6 breakpoints, regularly spaced from t=0 to t=3.2 sec. The stretch values are generated with a gaussian distribution centered on 1 (1:1 ratio, corresponding to no change of duration) and, in that specific instance, are mostly &gt; 1, which explains that the sound above is longer than the original. The largest stretch (x2.38 in duration) occurs at the end of the sound, ca. 2.56sec, which can be heard in the longer final last 2 syllables (/veeeer/saaaaaire/) in the extract above.  </p> <p>As above, one can use the <code>extract_pitch</code> utility to visualize the difference between the two files. Notice that, contrary to the <code>pitch</code> transform above, the actual pitch values in the two sounds are not changed, but only how they unfold in time. </p> visualize before and after transform<pre><code># extract pitch before transformation\ntimes_in,pitch_in = PhaseVocoder.extract_pitch(wave_in,sr)\n# extract pitch after transformation\ntimes_out,pitch_out = PhaseVocoder.extract_pitch(wave_out,sr)\n\n# display \nplt.plot(times_in, pitch_in, 'k:')\nplt.plot(times_out, pitch_out, 'k', label='post')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch (Hz)')\n\nplt.ylim([180,310])\n</code></pre> <p></p>"},{"location":"tutorials/speech/#batched-transforms","title":"Batched transforms","text":"<p>Instead of generating output files one at a time, CLEESE can be used to generate large numbers of manipulated files, each randomly generated using parameters specified in config files as above. This is achieve by pusing cleese.generate_stimuli <code>cleese.generate_stimuli(PhaseVocoder, input_file, config_file)</code>. Output files are not returned by the function, but directly written in <code>main.outPath</code>, and the number of output files generated is given by <code>main.numFiles</code>, all of which are found in the configuration file:</p> random_pitch_profile.toml<pre><code>[main]\n\n# output root folder\noutPath = \"./output/\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# apply transformation in series (True) or parallel (False)\nchain = true\n\n# transformations to apply\ntransf = [\"pitch\"]\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = true\n</code></pre> <p>The following code will create 10 random transformations of the <code>input_file</code>, each with random parameters generated from <code>config_file</code>, and store both files and parameters in the <code>outPath</code> folder designated in <code>config_file</code> </p> <p>Note</p> <p>If the <code>outPath</code> directory doesn't exist in your working directory, it will be created automatically. </p> batch transform<pre><code>input_file = \"./sounds/male_vraiment_flattened.wav\"\nconfig_file = \"./configs/random_pitch_profile.toml\"\n\n# CLEESE\ncleese.generate_stimuli(PhaseVocoder, input_file, config_file)\n</code></pre> <p>  Download audio   Download audio   Download audio   Download audio   ...</p>"},{"location":"tutorials/speech/#chained-transforms","title":"Chained transforms","text":"<p>CLEESE can process files with a series of transformations that follow each other, e.g. first time-stretch the file, then pitch-shift it. This is done by specifying keyword <code>chain = true</code> under the configuration section <code>[main]</code>, as well as the list of transformations to be applied, e.g. here <code>transf = ['pitch','stretch']</code>.  </p> chained_pitch_stretch.toml<pre><code>[main]\n\n# output root folder\noutPath = \"./output/\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# apply transformation in series (True) or parallel (False)\nchain = true\n\n# transformations to apply\ntransf = [\"pitch\", \"stretch\"]\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = true\n</code></pre> <p>The following code runs a chained transformation (notice the change of <code>config_file</code>) on 10 files, and stores them all in the <code>outPath</code> folder designated in <code>config_file</code></p> chained transform<pre><code>input_file = \"./sounds/male_vraiment_flattened.wav\"\nconfig_file = \"./configs/chained_pitch_stretch.toml\"\n\n# CLEESE\ncleese.generate_stimuli(PhaseVocoder, input_file, config_file)\n</code></pre> <p>  Download audio   Download audio   Download audio   Download audio  </p>"},{"location":"tutorials/speech/#advanced-use","title":"Advanced use","text":""},{"location":"tutorials/speech/#flattening-files","title":"Flattening files","text":"<p>When applying CLEESE to generate stimuli for reverse correlation, it is often advisable to use base stimuli that are as flat as possible (e.g., if randomizing pitch, start with a sound that has constant pitch). CLEESE can be used to flatten an existing recording, using the trick of not letting the tool generate its own random breakpoint function, but rather providing it with a custom function that inverts the natural pitch variations found in the original file. We demonstrate this with an original, non flattened recording of the word \"vraiment\". </p> <p>Start with a normal, non-flat recording of the same word ``vraiment'' as above: </p> <p>  Download audio </p> <p>The file has a soft, down-ward pitch contour, as show here</p> display original pitch<pre><code>input_file = \"./sounds/male_vraiment_original.wav\"\nwave_in, sr, _ = PhaseVocoder.wav_read(input_file)\ntimes_in,pitch_in = PhaseVocoder.extract_pitch(wave_in,sr, win=0.02, bounds=[50, 200])\nplt.plot(times_in, pitch_in, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch')\nplt.ylim([80,120])\n</code></pre> <p></p> <p>To flatten this existing contour, we construct a custom break-point function (bpf) that passes through the pitch shift values needed to shift the contour down to a constant pitch value, arbitrarily set here at 110Hz. </p> custom bpf<pre><code>mean_pitch = 110.\ndef difference_to_cents(pitch, ref_pitch):\n    if pitch &gt;0:\n        return -1200*np.log2(pitch/ref_pitch)\n    else:\n        return 1\nbpf_times = times_in\nbpf_val = np.array([difference_to_cents(hz, mean_pitch) for hz in pitch_in])\n# display original file\nplt.plot(1000*bpf_times, bpf_val, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('BPF')\nplt.plot([40,350],[0,0],'k:')\n</code></pre> <p></p> <p>We then apply this custom BPF to the original file, using <code>cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)</code> (passing audio data as input, because we don't need batch mode here). </p> apply bpf<pre><code>config_file = \"./configs/random_pitch_profile.toml\"\n\n# CLEESE\nbpf = np.column_stack((bpf_times,bpf_val))\nwave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)\n</code></pre> <p>  Download audio </p> <p>Compare pitch profile before and after transformation: </p> display resulting pitch<pre><code># display transformed file\ntimes_out,pitch_out = PhaseVocoder.extract_pitch(wave_out,sr, win=0.02, bounds=[100, 200])\nplt.plot(times_in, pitch_in, 'k:')\nplt.plot(times_out, pitch_out, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch (Hz)')\nplt.ylim([80,120])\n</code></pre> <p></p>"},{"location":"tutorials/speech/#using-custom-breakpoints","title":"Using custom breakpoints","text":"<p>Instead of generating linearly spaced time windows (or, as called here, breakpoints), CLEESE supports a list of externally provided time positions. To demonstrate this, we use CLEESE to stretch the duration of each note in the song \"Joyeux Anniversaire\" (which we already used above). </p> <p>  Download audio </p> <p>To find note boundaries, we can e.g. use an external audio editor such as Audacity, and measure time positions between notes as <code>[0.027, 0.634, 1.137, 1.647, 2.185, 2.649, 3.181]</code>.</p> <p></p> <p>We can then generate a breakpoint function with <code>cleese.create_BPF</code> which uses these time points and parameters loaded from the stretch config file <code>config_file</code>. This BPF can then be passed to <code>cleese.process_data</code> as argument. </p> process with custom breakpoints<pre><code>input_file = \"./sounds/female_anniversaire_isochrone.wav\"\nconfig_file = \"./configs/random_speed_profile.toml\"\n\nwave_in, sr, _ = PhaseVocoder.wav_read(input_file)\n\ntime_points = np.array([0.027, 0.634, 1.137, 1.647, 2.185, 2.649, 3.181]) # values found in audacity\nnum_points = len(time_points)\nbpf = PhaseVocoder.create_BPF(\n    'stretch',config_file,time_points,num_points,0)   \n\nwave_out,bpf_out = cleese.process_data(\n    PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)\n</code></pre> <p>The resulting file has random duration, but these changes of pace are aligned with note boundaries. </p> <p>  Download audio </p> display pitch before/after<pre><code>times_in,pitch_in = PhaseVocoder.extract_pitch(wave_in,sr)\nplt.plot(times_in, pitch_in, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch')\n\n# display transformed file\ntimes_out,pitch_out = PhaseVocoder.extract_pitch(wave_out,sr)\nplt.plot(times_out, pitch_out, 'b')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch')\n</code></pre> <p></p>"}]}
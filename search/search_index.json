{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>CLEESE is a Python toolbox to help the generation of randomized sound and image stimuli for neuroscience research. </p> <p>It provides a number of sound and image transformation algorithms (so-called <code>Engines</code>) able e.g. to create natural-sounding expressive variations around an original speech recording, or expressive variations on a human face. It also provides a config file interface to automatize the call to these algorithms, in order e.g. to easily create thousands of random variants from a single base file, which can serve as stimuli for neuroscience experiments. </p> <p>As of version v2.0.0, CLEESE is composed of two engines: <code>PhaseVocoder</code> and <code>Mediapipe</code>: </p> <ul> <li> <p><code>PhaseVocoder</code> allows one to create random fluctuations around an audio file\u2019s original contour of pitch, loudness, timbre and speed (i.e. roughly   defined, its prosody). One of its foreseen applications is the generation of random voice stimuli for reverse correlation experiments in the vein of Ponsot, Burred, Belin &amp; Aucouturier (2018) Cracking the social code of speech prosody using reverse correlation. PNAS, 115(15), 3972-3977.</p> </li> <li> <p><code>FaceWarp</code> uses mediapipe's Face Mesh API to introduce random or precomputed deformation in the expression of a   visage on an image. This engine was designed to produce batches of deformed faces for reverse correlation experiments in the vein of Jack, Garrod, Yu, Caldara, &amp; Schyns (2012). Facial expressions of emotion are not culturally universal. PNAS, 109(19), 7241-7244.</p> </li> </ul>"},{"location":"about/","title":"About CLEESE","text":"<p>CLEESE is a free, standalone Python module, distributed under an open-source MIT Licence on the FEMTO Neuro team github page. </p> <p>It was originally designed in 2018 by Juan Jos\u00e9 Burred, Emmanuel Ponsot and Jean-Julien Aucouturier at STMS Lab (IRCAM/CNRS/Sorbonne Universit\u00e9, Paris - France), and released on the IRCAM Forum plateform. As of 2021, CLEESE is now developped and maintained by the FEMTO Neuro Team at the FEMTO-ST Institute (CNRS/Universit\u00e9 Bourgogne Franche-Comt\u00e9) in Besan\u00e7on - France, and distributed on the team's github page. </p> <p>CLEESE's development was originally funded by the European Research Council (CREAM 335536, 2014-2019, PI: JJ Aucouturier), and has since then received support from Agence Nationale de la Recherche (ANR SEPIA, AND Sounds4Coma), Fondation pour l'Audition (DASHES) and R\u00e9gion Bourgogne-Franche Comt\u00e9 (ASPECT). </p>"},{"location":"about/#citing-cleese","title":"Citing CLEESE","text":"<p>If you use CLEESE in academic work, please cite it as : </p> <p>Burred, J. J., Ponsot, E., Goupil, L., Liuni, M., &amp; Aucouturier, J. J. (2019). CLEESE: An open-source audio-transformation toolbox for data-driven  experiments in speech and music cognition. PloS one, 14(4), e0205943.</p>"},{"location":"about/#cleese-contributors","title":"CLEESE contributors:","text":"<ul> <li>Juan Jos\u00e9 Burred (original development, Phase Vocoder Engine)  jjburred</li> <li>Emmanuel Ponsot (tool specification) </li> <li>JJ Aucouturier (software architecture)  jjau</li> <li>Lara Kermarec (Face warp engine)  nemirwen</li> <li>Paige Tuttosi (Documentation)  chocobearz</li> </ul>"},{"location":"api/face-warp/","title":"Face Warp","text":"<p>CLEESE's <code>FaceWarp</code> engine works by first identifying a set of landmarks on the visage present in the image. Then a gaussian distribution of deformation vectors is applied to a subset of landmarks, and the Moving Least Squares (MLS) algorithm is used to apply the deformation to the image itself. Alternatively, a precomputed set of deformations can also be provided.</p>"},{"location":"api/face-warp/#modes-of-operation","title":"Modes of operation","text":"<p>CLEESE can be used in different modes, depending on which function you call and how. </p>"},{"location":"api/face-warp/#batch-generation","title":"Batch generation","text":"<p>CLEESE has a dedicated function for batch treatments: <code>cleese.generate_stimuli</code>, which is used, in <code>Mediapipe</code>'s case, to generate a number of randomly deformed visages.</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\ncleese.generate_stimuli(Mediapipe, inputFile, configFile)\n</code></pre> <p>Two parameters have to be set by the user:</p> <ul> <li><code>inputFile</code>: the path to the base image, which can be any image format readable and writeable by <code>PIL</code>.</li> <li><code>configFile</code>: the path to the configuration script</li> </ul> <p>All the generation parameters for all treatments are set up in the configuration script that has to be edited or created by the user. An example of configuration script with parameters for all treatments is included with the toolbox: <code>cleese-mediapipe.toml</code>. Configuration parameters will be detailed below.</p> <p>For each run in batch mode, the toolbox generates the following folder structure, where <code>&lt;outPath&gt;</code> is specified in the parameter file:</p> <ul> <li><code>&lt;outPath&gt;/&lt;currentExperimentFolder&gt;</code>: main folder for the current generation experiment. The name <code>&lt;currentExperimentFolder&gt;</code> is automatically created from the current date and time. This folder contains:<ul> <li><code>&lt;baseImage.ext&gt;</code>: a copy of the base image used for the current experiment</li> <li><code>*.toml</code>: a copy of the configuration script used for the current experiment</li> <li><code>&lt;baseimage&gt;.xxxxxxxx.&lt;ext&gt;</code>: the generated deformed image, where <code>xxxxxxxx</code> is a running number (e.g.: <code>monalisa.00000001.jpg</code>)</li> <li><code>&lt;baseimage&gt;.xxxxxxxx.dfmxy</code>: the generated deformation vectors, in CSV format, for the generated stimulus (e.g.: <code>monalisa.00000001.dfmxy</code>)</li> <li><code>&lt;baseimage&gt;.landmarks.txt</code>: the list of all landmarks positions as detected on the original image, in ASCII format, readable with <code>numpy.loadtxt</code>.</li> </ul> </li> </ul>"},{"location":"api/face-warp/#array-input-and-output","title":"Array input and output","text":"<p>CLEESE can also provide a single result, based on data loaded previously in a script or directly loading a file.</p> <p>From array: </p> <p>Here, you can also use <code>PIL.Image</code>'s <code>np.array(Image.open(\"path/image.jpg\").convert(\"RGB\"))</code> to load the image. The <code>Face Mesh</code> API requires that the image be in RGB format.</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\nimg = Mediapipe.load_file(inputFile)\ndeformedImg = cleese.process_data(Mediapipe, img, configFile)\n</code></pre> <p>From file: <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ninputFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\ndeformedImg = cleese.process_file(Mediapipe, inputFile, configFile)\n</code></pre></p> <p>In both of those cases, no files or folder structures are generated.</p>"},{"location":"api/face-warp/#applying-a-deformation","title":"Applying a deformation","text":"<p>Both <code>cleese.process_data</code> and <code>cleese.process_file</code> function allow for applying a precomputed set of deformations instead of generating them randomly. CLEESE can accept both <code>.dfmxy</code> deformations (absolute cartesian deformation vectors), and <code>.dfm</code> deformations (deformation vectors mapped onto a triangulation of face landmarks (dlib landmarks indices)).</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ndfmxyFile = 'path_to_dfmxy.dfmxy'\ndfmFile = 'path_to_dfm.dfm'\nimageFile  = 'path_to_input_image.jpg'\nconfigFile = 'path_to_config_file.toml'\n\n# .dfmxy processing\ndfmxy = Mediapipe.load_dfmxy(dfmxyFile)\nimg = cleese.process_file(Mediapipe,\n                          imageFile,\n                          configFile,\n                          dfmxy=dfmxy)\n\n# .dfm processing\ndfm = Mediapipe.load_dfm(dfmFile)\nimg = cleese.process_file(Mediapipe,\n                          imageFile,\n                          configFile,\n                          dfm=dfm)\n</code></pre>"},{"location":"api/face-warp/#converting-deformation-files","title":"Converting deformation files","text":"<p>Other face deformation tools developed by our team use the <code>.dfm</code> deformation file format, more suited to applying the same deformation to an arbitrary face. However, by its use of barycentric coordinates in a landmarks triangulation, it isn't suited to any post or pre-processing, which is an area where <code>.dfmxy</code> shines. As a result, CLEESE's Mediapipe provides a way to convert a given, <code>.dfmxy</code> to <code>.dfm</code>, provided you also have the original landmarks on hand:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import Mediapipe\n\ndfmxyFile = 'path_to_dfmxy.dfmxy'\ndfmFile = 'path_to_dfm.dfm'\nlandmarksFile = 'path_to_landmarks.txt'\n\nimg = cleese.dfmxy_to_dfm(dfmxyFile,\n                          landmarksFile,\n                          output_dfm_file=dfmFile)\n</code></pre>"},{"location":"api/face-warp/#configuration","title":"Configuration","text":"<p>The following parameters are used to configure the <code>Mediapipe</code> engine:</p> <pre><code>[mediapipe.random_gen]\n# Indices of the landmarks to be modified, using Dlib's 68 landmarks indexing\nlandmarks.dlib = []\n\n# Indices of the landmarks to be modified, using Mediapipe's 468 landmarks indexing\nlandmarks.mediapipe = [61, 40, 78, 91, 270, 308, 321, 291]  # lips corners\n\n# Sets of landmarks to be modified, using precomputed sets\n# \"dlib-eyebrow-right\", \"dlib-eyebrow-left\", \"dlib-nose\",\n# \"dlib-eye-right\", \"dlib-eye-left\", \"dlib-outer-lips\",\n# \"dlib-inner-lips\", \"dlib-lips\", etc...\n# See cleese/engines/mediapipe.py for a full list\nlandmarks.presets = [\"dlib-lips\"]\n\n# Covariance matrix used to generate the gaussian distribution of landmarks\n# offsets. It is scaled according to the height of the detected face. As a\n# result, the amount of deformation should be resolution-invariant.\ncovMat = [[0.0002, 0.0], [0.0, 0.0002]]\n\n[mediapipe.mls]\n# Alpha parameter of the MLS deformation.\n# Affects how much the deformation \"spreads\" from the landmarks\nalpha = 1.2\n\n[mediapipe.face_detect]\n# Minimum face detection confidence\nthreshold = 0.5\n</code></pre> <ul> <li><code>mediapipe.random_gen.landmarks</code>: Selection of landmarks on which to apply a deformation. Then can be defined in a few different ways:<ul> <li><code>landmarks.dlib</code>: Array of landmark indices, using dlib's Multi-PIE 68 landmarks indexing (see figure below).</li> <li><code>landmarks.mediapipe</code>: Array of landmark indices, using mediapipe's 468 vertices face mesh (see Google's documentation here)</li> </ul> </li> <li><code>landmarks.presets</code>: Array of name of landmarks presets. For now, only subset of dlib's indices are implemented.</li> <li><code>covMat</code>: The covariance matrix used when drawing the distribution of deformation vectors. the unit vector is scaled according to height of the detected face, to enable for scale-invariant deformations.</li> <li><code>mediapipe.mls.alpha</code>: Moving Least Squares (MLS) algorithm alpha parameter, affecting the \"spread\" of the deformation.</li> <li><code>mediapipe.face_detect.threshold</code>: Threshold for the confidence metric of mediapipe's face detector. Ajust if faces aren't detected, or if things that aren't faces are detected.</li> </ul> Dlib's 68 Multi-PIE landmarks, 1-indexed."},{"location":"api/general/","title":"General API","text":""},{"location":"api/general/#common-configuration","title":"Common configuration","text":"<p>In order to configure the different tools it provides, CLEESE uses <code>toml</code> configuration files. Most sections in these file are specific to certain engines, but here are a few variables shared among all engines:</p> <pre><code>[main]\n\n# output root folder\noutPath = \"./CLEESE_output_data/\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = true\n</code></pre> <p>Enabling the <code>generateExpFolder</code> option will generate a new folder inside <code>outPath</code> for each subsequent experiment. Whereas if this option is disabled, all experiment results are written directly in <code>outPath</code>.</p>"},{"location":"api/phase-vocoder/","title":"PhaseVocoder Engine","text":"<p>CLEESE's <code>PhaseVocoder</code> engine operates by generating a set of random breakpoint functions (BPFs) in the appropriate format for each treatment, which are then passed to the included spectral processing engine (based on an implementation of the Phase Vocoder algorithm) with the corresponding parameters. Alternatively, the BPFs can be externally created by the user, and so it can also be used as a Phase Vocoder-based effects unit.</p>"},{"location":"api/phase-vocoder/#modes-of-operation","title":"Modes of operation","text":"<p>CLEESE can be used in several different modes, depending on how the main processing function is called. Examples of several typical usage scenarios are included in the example script \\texttt{run_cleese.py}.</p>"},{"location":"api/phase-vocoder/#batch-generation","title":"Batch generation","text":"<p>In batch mode, CLEESE's <code>PhaseVocoder</code> engine generates many random modifications from a single input sound file, called the base sound. It can be launched as follows:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile)\n</code></pre> <p>Two parameters have to be set by the user:</p> <ul> <li> <p><code>inputFile</code>: the path to the base sound, which has to be a mono sound in WAV format.</p> </li> <li> <p><code>configFile</code>: the path to the configuration file</p> </li> </ul> <p>All the generation parameters for all treatments are set up in the configuration file that has to be edited or created by the user. An example of configuration file with parameters for all treatments is included with the toolbox: <code>cleese-phase-vocoder.toml</code>. Configuration parameters will be detailed below.</p> <p>For each run in batch mode, the toolbox generates the following folder structure, where <code>&lt;outPath&gt;</code>} is specified in the parameter file:</p> <ul> <li><code>&lt;outPath&gt;/&lt;currentExperimentFolder&gt;</code>: main folder for the current generation experiment. The name <code>&lt;currentExperimentFolder&gt;</code> iscautomatically created from the current date and time. This folder contains:<ul> <li><code>&lt;baseSound&gt;.wav</code>: a copy of the base sound used for the current experiment</li> <li><code>*.toml</code>: a copy of the configuration script used for the current experiment</li> <li>one subfolder for each one of the performed treatments, which can be either <code>pitch</code>,  <code>stretch</code>, <code>gain</code>, <code>eq</code>, or a combination (chaining) of them. Each of them contains, for each generated stimulus:<ul> <li><code>&lt;baseSound&gt;.xxxxxxxx.&lt;treatment&gt;.wav</code>: the generated stimulus, where <code>xxxxxxxx</code> is a running number (e.g.: <code>cage.00000001.stretch.wav</code>)</li> <li><code>&lt;baseSound&gt;.xxxxxxxx.&lt;treatment&gt;BPF.txt</code>: the generated BPF, in ASCII format, for the generated stimulus (e.g.: <code>cage.00000001.stretchBPF.txt</code>)</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/phase-vocoder/#passing-a-given-bpf","title":"Passing a given BPF","text":"<p>When passing the <code>BPF</code> argument to <code>cleese.generate_stimuli</code>, it is possible to impose a given BPF with a certain treatment to an input file. In this way, the toolbox can be used as a traditional effects unit.</p> <pre><code>import numpy as np\n\nimport cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ngivenBPF = np.array([[0.,0.],[3.,500.]])\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile, BPF=givenBPF)\n</code></pre> <p>The <code>BPF</code> argument can be either:  - a numpy array containing the BPF  - a scalar, in which case the treatment performed is static</p> <p>In this usage scenario, only one file is output, stored at the <code>&lt;outPath&gt;</code> folder, as specified in the configuration file.</p>"},{"location":"api/phase-vocoder/#passing-a-given-time-vector","title":"Passing a given time vector","text":"<p>Instead of passing a full BPF (time+values), it is also possible to just pass a given time vector, containing the time instants (in seconds), at which the treatments change. The amount of modification will be randomly generated, but they will always happen at the given time tags. This might be useful to perform random modifications at specific onset locations, previously obtained manually or automatically.</p> <p>The time vector is passed via the <code>timeVec</code> argument:</p> <pre><code>import numpy as np\n\nimport cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\ngivenTimeVec = np.array([0.1,0.15,0.3])\ncleese.generate_stimuli(PhaseVocoder, inputFile, configFile, timeVec=givenTimeVec)\n</code></pre>"},{"location":"api/phase-vocoder/#array-input-and-output","title":"Array input and output","text":"<p>Instead of providing a file name for the input sound, it is possible to pass a Numpy array containing the input waveform. In this case, the main function will provide as output both the modified sound and the generated BPF as Numpy arrays. No files or folder structures are created as output:</p> <pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n\ninputFile  = 'path_to_input_sound.wav'\nconfigFile = 'path_to_config_file.toml'\n\nwaveIn,sr,__ = PhaseVocoder.wavRead(inputFile)\nwaveOut,BPFout = cleese.process_data(PhaseVocoder,\n                                     waveIn,\n                                     configFile,\n                                     sample_rate=sr)\n</code></pre> <p>Note that the sampling rate <code>sample_rate</code> has to be passed as well! Like when passing a BPF, only a single sound is generated.</p>"},{"location":"api/phase-vocoder/#configuration-file","title":"Configuration file","text":"<p>All the generation parameters are set in the configuration file. Please refer to the included configuration script <code>cleese-phase-vocoder.toml</code> for an example.</p>"},{"location":"api/phase-vocoder/#main-parameters","title":"Main parameters","text":"<p>The main parameters are set as follows:</p> <pre><code># apply transformation in series (True) or parallel (False)\nchain = true\n\n# transformations to apply\ntransf = [\"stretch\", \"pitch\", \"eq\", \"gain\"]\n\n[analysis]\n# analysis window length in seconds\n# (not to be confused with the BPF processing window lengths)\nwindow.len = 0.04\n\n# number of hops per analysis window\noversampling = 8\n</code></pre> <p>If <code>chain</code> is set to <code>true</code>, the transformations specified in the <code>transf</code> list will be applied as a chain, in the order implied by the list. For instance, the list <code>['stretch','pitch','eq','gain']</code> will produce the output folders <code>stretch</code>, <code>stretch_pitch</code>, <code>stretch_pitch_eq</code> and <code>stretch_pitch_eq_gain</code>, each one containing an additional step in the process chain.</p> <p>If <code>chain</code> is set to <code>false</code>, the transformations will be in parallel (i.e. all starting from the original sound file), producing the output folders <code>stretch</code>, <code>pitch</code>, <code>eq</code> and <code>gain</code>.</p>"},{"location":"api/phase-vocoder/#common-parameters","title":"Common parameters","text":"<p>The following parameters are shared by all treatments, but can take different values for each of them. </p> <p>In the following, <code>&lt;treatment&gt;</code> has to be replaced by one of the strings in <code>['stretch','pitch','eq','gain']</code>:</p> <pre><code># common treatment parameters\n[&lt;treatment&gt;]\n# BPF window in seconds. If 0 : static transformation\nwindow.len = 0.11\n\n# number of BPF windows. If 0 : static transformation\nwindow.count = 6\n\n# 's': force winlength in seconds,'n': force number of windows (equal length)\nwindow.unit = 'n'\n\n# standard deviation (cents) for each BPF point of the random modification\nstd = 300\n\n# truncate distribution values (factor of std)\ntrunc = 1\n\n# type of breakpoint function:\n#      'ramp': linear interpolation between breakpoints\n#      'square': square BPF, with specified transition times at edges\nBPFtype = 'ramp'\n\n# in seconds: transition time for square BPF\ntrTime = 0.02\n</code></pre> <ul> <li> <p><code>window.len</code>: Length in seconds of the treatment window (i.e., the window used to generate the timestamps in the BPFs). It should be longer than         <code>analysis.window.len</code>. This is only used if <code>window.unit = 's'</code> (see below).     Note: static treatment: if <code>window.length = 0</code>, the treatment is static (i.e. it doesn't change with time - an all-flat BPF).</p> </li> <li> <p><code>window.count</code>: Total number of treatment windows. This is only used if <code>window.unit = 'n'</code> (see below).     Note: static treatment: if <code>window.count = 0</code>, the treatment is static (flat BPF).</p> </li> <li><code>window.unit</code>: Whether to enforce window length in seconds (<code>s</code>) or integer number of windows (<code>n</code>).</li> <li><code>std</code>: Standard deviation of a Gaussian distribution from which the random values at each timestamp of the BPFs will be sampled. The unit of the std is specific to each treatment: <ul> <li>for <code>pitch</code>: cents</li> <li>for <code>eq</code> and <code>gain</code>: amplitude dBs</li> <li>for <code>stretch</code>: stretching factor (~&gt;1: expansion, &lt;1: compression)</li> </ul> </li> <li><code>trunc</code>: Factor of the std above which distribution samples are not allowed. If a sample is greater than +/- <code>std * trunc</code>, a new random value         is sampled at that point.</li> <li><code>BPFtype</code>: Type of BPF. Can be either <code>ramp</code> or <code>square</code></li> <li><code>trTime</code>: For BPFs of type <code>square</code>, length in seconds of the transition phases.</li> </ul>"},{"location":"api/phase-vocoder/#bpfs","title":"BPFs","text":"<p>In CLEESE, sound transformations can be time-varying: the amount of modification (e.g. the pitch shifting or time stretching factors) can dynamically change over the duration of the input sound file. The break-point functions (BPFs) determine how these modifications vary over time. For the <code>pitch</code>, <code>stretch</code> and <code>gain</code> treatments, BPFs are one-dimensional (temporal). For the <code>eq</code> treatment, BPFs are two-dimensional (spectro-temporal).</p> <p>As has been seen, BPFs can be either randomly generated by CLEESE or provided by the user.</p>"},{"location":"api/phase-vocoder/#temporal-bpfs","title":"Temporal BPFs","text":"<p>For the <code>pitch</code>, <code>stretch</code> and <code>gain</code> treatments, BPFs are temporal: they are two-column matrices with rows of the form: <pre><code>time, value\nt0, v0\nt1, v1\n... \ntn, vn\n</code></pre> where <code>time</code> is in seconds, and <code>value</code> is in the same units than the transform's <code>std</code> parameter (ex. cents for <code>pitch</code>).</p> Ramp BPF with window specified in seconds Ramp BPF with window specified in number Square BPF with window specified in seconds Square BPF with window specified in number <p>CLEESE can randomly generate one-dimensional BPFs of two types:</p> <ul> <li> <p>Ramps <code>BPFtype = 'ramp'</code>: the BPF is interpreted as a linearly interpolated function. The result is that the corresponding sound parameter is changed gradually (linearly) between timestamps. Examples are shown above for a treatment window defined in seconds (<code>window.unit = 's'</code>), and for a treatment window defined in terms of window number (<code>window.unit = 'n'</code>). Note that in the first case, the length of the last window depends on the length of the input sound. In the second case, all windows have the same length.</p> </li> <li> <p>Square (<code>BPFtype = 'square'</code>): the BPF is a square wave with sharply sloped transitions, whose length is controlled by <code>trTime</code>. Examples are shown above for a treatment window defined in seconds (<code>window.unit = 's'</code>), and for a treatment window defined in terms of window number (<code>window.unit = 'n'</code>).</p> </li> </ul>"},{"location":"api/phase-vocoder/#spectro-temporal-bpfs","title":"Spectro-temporal BPFs","text":"<p>The <code>eq</code> treatment performs time-varying filtering over a number of determined frequency bands. It thus expects a spectro-temporal (two-dimensional) BPF whose rows are defined as follows:</p> <pre><code>time numberOfBands freq1 value1 freq2 value2 freq3 value3 \nt0, m, f1, v1_0, f2, v2_0, ..., fm, vm_0\nt1, m, f1, v1_1, f2, v2_1, ..., fm, vm_1\n...\ntn, m, f1, v1_n, f2, v2_n, ..., fm, vm_n\n</code></pre> <p>The temporal basis can again be generated as <code>ramp</code> or <code>square</code>. In contrast, in the frequency axis, points are always interpolated linearly. Thus, a spectro-temporal BPF can be interpreted as a time-varying piecewise-linear spectral envelope.</p>"},{"location":"api/phase-vocoder/#treatments","title":"Treatments","text":""},{"location":"api/phase-vocoder/#time-stretching-stretch","title":"Time stretching (<code>stretch</code>)","text":"<p>This treatment stretches or compresses locally the sound file without changing the pitch, according to the current stretching factor (oscillating around 1) at the current timestamp. This is the only treatment that changes the duration of the output compared to the base sound. The used algorithm is a phase vocoder with phase locking based on frame-wise peak picking.</p>"},{"location":"api/phase-vocoder/#pitch-shifting-pitch","title":"Pitch shifting (<code>pitch</code>)","text":"<p>The BPF is used to transpose up and down the pitch of the sound, without changing its duration. The used algorithm is a phase vocoder with phase locking based on frame-wise peak picking, followed by resampling on a window-by-window basis.</p>"},{"location":"api/phase-vocoder/#time-varying-equalization-eq","title":"Time-varying equalization (<code>eq</code>)","text":"<p>This treatment divides the spectrum into a set of frequency bands, and applies random amplitudes to the bands. The definition of band edges is constant, the amplitudes can be time-varying. The corresponding BPF is thus two-dimensional</p> <p>There are two possible ways to define the band division:</p> <ul> <li><code>Linear</code> division into a given number of bands between 0 Hz and Nyquist.</li> <li>Division according to a <code>mel</code> scale into a given number of bands. Note that it it possible to specify any number of filters (less or more than         the traditional 40 filters for mel cepstra.</li> </ul> <p>These settings are defined by the following treatment-specific parameters:</p> <pre><code>[eq]\nscale = 'mel'  # mel, linear\nband.count = 10\n</code></pre>"},{"location":"api/phase-vocoder/#time-varying-gain-gain","title":"Time-varying gain (<code>gain</code>)","text":"<p>For gain or level randomization, the BPF is interpolated and interpreted as an amplitude modulator. Note that the corresponding standard deviation is specified in dBs (base-10 logarithm). If the resulting output exceeds the maximum float amplitude of <code>1.0</code>, the whole output signal is normalized. </p>"},{"location":"tutorials/speech/","title":"Tutorial: random speech generation","text":"<p>This tutorial shows how to use CLEESE's <code>PhaseVocoder</code> engine to generate a arbitrary number of expressive variations around an original speech recording.</p>"},{"location":"tutorials/speech/#preambule","title":"Preambule","text":""},{"location":"tutorials/speech/#verify-your-installation","title":"Verify your installation","text":"<p>Before starting, please verify that you have a working CLEESE installation, by running the following cell which you return without error. </p> <p>import cleese<pre><code>import cleese_stim as cleese\nfrom cleese_stim.engines import PhaseVocoder\n</code></pre> Check the installation instructions if needed. </p>"},{"location":"tutorials/speech/#useful-imports","title":"Useful imports","text":"<p>The following code imports all the python packages that are needed in the rest of this tutorial (which you can <code>pip install</code> if you don't have them already). </p> extra imports<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"tutorials/speech/#useful-files","title":"Useful files","text":"<p>In the following, we'll be a number of files which you'll first need to download and store in your path at the indicated place</p> <ul> <li>male_vraiment_flattened.wav <code>./sounds/male_vraiment_flattened.wav</code> </li> <li>random_pitch_profile.toml <code>./configs/random_pitch_profile.toml</code></li> <li>female_anniversaire_isochrone.wav <code>./sounds/female_anniversaire_isochrone.wav</code></li> <li>random_speed_profile.toml <code>./configs/random_speed_profile.toml</code></li> <li>chained_pitch_stretch.toml :material-arrow-right <code>./configs/chained_pitch_stretch.toml</code></li> <li>male_vraiment_original.wav <code>./sounds/male_vraiment_original.wav</code> </li> </ul>"},{"location":"tutorials/speech/#basic-sound-manipulation-with-cleese","title":"Basic sound manipulation with CLEESE","text":""},{"location":"tutorials/speech/#random-pitch-profile-in-a-single-utterance","title":"Random pitch profile in a single utterance","text":"<p>The most basic usage scenario of CLEESE is to input a single recording (ex. the French word \"vraiment\" - \"really\", recorded by a single male speaker)</p> <p>  Download audio </p> <p>and use CLEESE to transform the sound with a random pitch profile. This, like all cleese operations, is done by passing to the main cleese function <code>cleese.process_data</code> a configuration file (random_pitch_profile.toml) which specifies the manipulation we want. Here: cut the file in <code>pitch.window.count = 6</code> time segments, draw a random pitch shift factor at each segment boundary from a Gaussian distribution centered on 0 and standard deviation <code>pitch.std = 300</code>cents, and interpolate between segment boundaries using linear <code>pitch.BPFType = \"ramp\"</code>. (See PhaseVocoder documentation for more information)</p> random_pitch_profile.toml<pre><code>[pitch]\n# pitch transposition window in seconds. If 0 : static transformation\nwindow.len = 0.11\n\n# number of pitch transposition windows. If 0 : static transformation\nwindow.count = 6\n\n# 's': force winlength in seconds,'n': force number of windows (equal length)\nwindow.unit = 'n'\n\n# standard deviation (cents) for random transposisiton (Gaussian distrib for now)\nstd = 300\n\n# truncate distribution values (factor of std)\ntrunc = 1\n\n# type of breakpoint function:\n#      'ramp': linear interpolation between breakpoints\n#      'square': square BPF, with specified transition times at edges\nBPFtype = 'ramp'\n\n# in s: transition time for square BPF\ntrTime = 0.02\n</code></pre> <p>The following code is pretty much all there is to call: <code>cleese.process_data</code> takes in the <code>Engine</code> that is called on to do the transformation (here, <code>PhaseVocoder</code> - see the Image tutorial for a similar call to image-transformation engine <code>FaceWarp</code>), the array <code>wave_in</code> of audio data obtained from <code>PhaseVocoder.wav_read</code> and its sampling rate <code>sr</code>, and the path to the configuration file <code>config_file</code> that tells the <code>PhaseVocoder</code> engine what to do with it all. </p> randomize pitch<pre><code>input_file = \"./sounds/male_vraiment_flattened.wav\"\nconfig_file = \"./configs/random_pitch_profile.toml\"\n\n# read input wavefile, and extract pitch for display (unnecessary for cleese.process below)\nwave_in, sr, _ = PhaseVocoder.wav_read(input_file)\n\n# transform sound\nwave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr)\n\n# save file if necessary\noutput_file = \"./sounds/male_vraiment_flattened_transformed.wav\"\nPhaseVocoder.wav_write(wave_out, output_file, sr)\n</code></pre> <p>  Download audio </p> <p>CLEESE's <code>PhaseVocoder</code> includes a utility for extracting pitch in speech/audio files (<code>PhaseVocoder.extract_pitch</code>), which uses the YIN pitch extraction algorithm, and can be used to visualize the pitch profile of sounds before and after manipulation. This is just for visualization purposes, and isn't necessary for the working of the main <code>cleese.process</code> function above. </p> visualize pitch before/after<pre><code># extract pitch before transformation\npitch_in,times_in = PhaseVocoder.extract_pitch(wave_in,sr)\n\n# extract pitch after transformation\npitch_out,times_out = PhaseVocoder.extract_pitch(wave_out,sr)\n\n# display \nplt.plot(times_in, pitch_in, 'k:', label='pre')\nplt.plot(times_out, pitch_out, 'k', label='post')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch (Hz)')\nplt.ylim([70,120])\n</code></pre> <p></p>"},{"location":"tutorials/speech/#random-speed-profile-in-a-song","title":"Random speed profile in a song","text":"<p>CLEESE can process longer files than a single word and, instead of manipulating pitch, can manipulate the duration of each portion of the file. To demonstrate this, we use CLEESE to randomly stretch each note in a recording of a song (the French song \"Joyeux Anniversaire\" / \"Happy Birthday\", sung by a female singer)</p> <p>  Download audio </p> <p>This, as above, is done by passing to <code>cleese.process_data</code> a configuration file which specifies the manipulation we want. Here: cut the file in <code>stretch.window.len = 0.5</code> second time segments, draw a random stretch shift factor at each segment boundary from a Gaussian distribution centered on 1.0 and standard deviation <code>stretch.std = 1.5</code> (where factors &gt;1 correspond to a time stretch, and factors &lt;1 correspond to a time compression), and interpolate between segment boundaries using linear <code>stretch.BPFType = \"ramp\"</code>. </p> random_speed_profile.toml<pre><code>[stretch]\n\nwindow.len = 0.1\nwindow.count = 5\nwindow.unit = 'n'\n\n# stretching factor. &gt;1: expansion, &lt;1: compression\nstd = 1.5\ntrunc = 1\nBPFtype = 'ramp'\ntrTime = 0.05\n</code></pre> <p>The following code runs the transformation</p> randomize duration<pre><code>input_file = \"./sounds/female_anniversaire_isochrone.wav\"\nconfig_file = \"./configs/random_speed_profile.toml\"\n\n# read input wavefile\nwave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n\n# CLEESE\nwave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr)\n\n# save file if necessary\noutput_file = \"./sounds/female_anniversaire_isochrone_transformed.wav\"\nPhaseVocoder.wav_write(wave_out, output_file, sr)\n</code></pre> <p>  Download audio </p> <p>Display pre and post pitch profile: notice pitch values weren't changed, but only how they appear in time)</p> visualize before transform<pre><code># extract pitch before transformation\npitch_in,times_in = PhaseVocoder.extract_pitch(wave_in,sr)\n\n# display \nplt.plot(times_in, pitch_in, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch (Hz)')\nplt.ylim([70,120])\n</code></pre> <p></p> visualize after transform<pre><code># extract pitch after transformation\npitch_out,times_out = PhaseVocoder.extract_pitch(wave_out,sr)\n\n# display \nplt.plot(times_out, pitch_out, 'k', label='post')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch (Hz)')\nplt.ylim([70,120])\n</code></pre> <p></p>"},{"location":"tutorials/speech/#batched-transforms","title":"Batched transforms","text":"<p>Instead of generating output files one at a time, CLEESE can be used to generate large numbers of manipulated files, each randomly generated using parameters specified in config files as above. This is achieve by pusing cleese.generate_stimuli <code>cleese.generate_stimuli(PhaseVocoder, input_file, config_file)</code>. Output files are not returned by the function, but directly written in <code>main.outPath</code>, and the number of output files generated is given by <code>main.numFiles</code>, all of which are found in the configuration file:</p> random_pitch_profile.toml<pre><code>[main]\n\n# output root folder\noutPath = \"./output/\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# apply transformation in series (True) or parallel (False)\nchain = true\n\n# transformations to apply\ntransf = [\"pitch\"]\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = true\n</code></pre> <p>The following code will create 10 random transformations of the <code>input_file</code>, each with random parameters generated from <code>config_file</code>, and store both files and parameters in the <code>outPath</code> folder designated in <code>config_file</code> </p> <p>Warning</p> <p>Depending on how you run this code, you may want to ensure the <code>outPath</code> folder exists in your path before running this code </p> batch transform<pre><code>input_file = \"./sounds/male_vraiment_flattened.wav\"\nconfig_file = \"./configs/random_pitch_profile.toml\"\n\n# CLEESE\ncleese.generate_stimuli(PhaseVocoder, input_file, config_file)\n</code></pre> <p>  Download audio   Download audio   Download audio   Download audio  </p>"},{"location":"tutorials/speech/#chained-transforms","title":"Chained transforms","text":"<p>CLEESE can process files with a series of transformations that follow each other, e.g. first time-stretch the file, then pitch-shift it. This is done by specifying keyword <code>chain = true</code> under the configuration section <code>[main]</code>, as well as the list of transformations to be applied, e.g. here <code>transf = ['pitch','stretch']</code>.  </p> chained_pitch_stretch.toml<pre><code>[main]\n\n# output root folder\noutPath = \"./output/\"\n\n# number of output files to generate (for random modifications)\nnumFiles = 10\n\n# apply transformation in series (True) or parallel (False)\nchain = true\n\n# transformations to apply\ntransf = [\"pitch\", \"stretch\"]\n\n# generate experiment folder with name based on current time\ngenerateExpFolder = true\n</code></pre> <p>The following code runs a chained transformation (notice the change of <code>config_file</code>) on 10 files, and stores them all in the <code>outPath</code> folder designated in <code>config_file</code></p> chained transform<pre><code>input_file = \"./sounds/male_vraiment_flattened.wav\"\nconfig_file = \"./configs/chained_pitch_stretch.toml\"\n\n# CLEESE\ncleese.generate_stimuli(PhaseVocoder, input_file, config_file)\n</code></pre> <p>  Download audio   Download audio   Download audio   Download audio  </p>"},{"location":"tutorials/speech/#advanced-use","title":"Advanced use","text":""},{"location":"tutorials/speech/#flattening-files","title":"Flattening files","text":"<p>When applying CLEESE to generate stimuli for reverse correlation, it is often advisable to use base stimuli that are as flat as possible (e.g., if randomizing pitch, start with a sound that has constant pitch). CLEESE can be used to flatten an existing recording, using the trick of not letting the tool generate its own random breakpoint function, but rather providing it with a custom function that inverts the natural pitch variations found in the original file. We demonstrate this with an original, non flattened recording of the word \"vraiment\". </p> <p>Start with a normal, non-flat recording of the same word ``vraiment'' as above: </p> <p>  Download audio </p> <p>The file has a soft, down-ward pitch contour, as show here</p> display original pitch<pre><code>input_file = \"./sounds/male_vraiment_original.wav\"\nwave_in, sr, _ = PhaseVocoder.wav_read(input_file)\npitch_in,times_in = PhaseVocoder.extract_pitch(wave_in,sr, win=0.02, bounds=[50, 200])\nplt.plot(times_in, pitch_in, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch')\n</code></pre> <p></p> <p>To flatten this existing contour, we construct a custom break-point function (bpf) that passes through the pitch shift values needed to shift the contour down to a constant pitch value, arbitrarily set here at 110Hz. </p> custom bpf<pre><code>mean_pitch = 110.\ndef difference_to_cents(pitch, ref_pitch):\n    if pitch &gt;0:\n        return -1200*np.log2(pitch/ref_pitch)\n    else:\n        return 1\nbpf_times = times\nbpf_val = np.array([difference_to_cents(hz, mean_pitch) for hz in pitch])\n# display original file\nplt.plot(1000*bpf_times, bpf_val, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('BPF')\n</code></pre> <p></p> <p>We then apply this custom BPF to the original file, using <code>cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)</code> (passing audio data as input, because we don't need batch mode here). </p> apply bpf<pre><code>config_file = \"./configs/random_pitch_profile.toml\"\n\n# CLEESE\nbpf = np.column_stack((bpf_times,bpf_val))\nwave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)\n</code></pre> <p>  Download audio </p> <p>Compare pitch profile before and after transformation: </p> display resulting pitch<pre><code># display transformed file\npitch_out,times_out = PhaseVocoder.extract_pitch(wave_out,sr, win=0.005, bounds=[50, 200])\nplt.plot(times_in, pitch_in, 'k')\nplt.plot(times_out, pitch_out, 'b')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch')\n</code></pre> <p></p>"},{"location":"tutorials/speech/#using-custom-breakpoints","title":"Using custom breakpoints","text":"<p>Instead of generating linearly spaced time windows (or, as called here, breakpoints), CLEESE supports a list of externally provided time positions. To demonstrate this, we use CLEESE to stretch the duration of each note in the song \"Joyeux Anniversaire\" (which we already used above). </p> <p>  Download audio </p> <p>To find note boundaries, we can e.g. use an external audio editor such as Audacity, and measure time positions between notes as <code>[0.027, 0.634, 1.137, 1.647, 2.185, 2.649, 3.181]</code>.</p> <p></p> <p>We can then generate a breakpoint function with <code>cleese.create_BPF</code> which uses these time points and parameters loaded from the stretch config file <code>config_file</code>. This BPF can then be passed to <code>cleese.process_data</code> as argument. </p> process with custom breakpoints<pre><code>input_file = \"./sounds/female_anniversaire_isochrone.wav\"\nconfig_file = \"./configs/random_speed_profile.toml\"\n\nwave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n\ntime_points = np.array([0.027, 0.634, 1.137, 1.647, 2.185, 2.649, 3.181]) # values found in audacity\nnum_points = len(time_points)\nbpf = PhaseVocoder.create_BPF(\n    'stretch',config_file,time_points,num_points,0)   \n\nwave_out,bpf_out = cleese.process_data(\n    PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)\n</code></pre> <p>  Download audio </p> display pitch before/after<pre><code>pitch_in,times_in = PhaseVocoder.extract_pitch(wave_in,sr)\nplt.plot(times_in, pitch_in, 'k')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch')\n\n# display transformed file\npitch_out,times_out = PhaseVocoder.extract_pitch(wave_out,sr)\nplt.plot(times_out, pitch_out, 'b')\nplt.xlabel('time in file (ms)')\nplt.ylabel('pitch')\n</code></pre> <p></p>"}]}
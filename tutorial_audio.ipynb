{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> CLEESE: A SHORT TUTORIAL </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cleese](tutorial/pics/silly-walk.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEESE (\"Ministry of Silly Speech\") is a sound and image manipulation tool designed to generate an infinite number of possible stimuli; be it natural-sounding expressive variations around an original speech recording, or variations on the expression of a human face.\n",
    "\n",
    "More precisely, CLEESE is currently composed of two engines: `PhaseVocoder` and `Mediapipe`.\n",
    "* `PhaseVocoder` allows one to create random fluctuations around an audio file’s original contour of pitch, loudness, timbre and speed (i.e. roughly defined, its prosody). One of its foreseen applications is the generation of very many random voice stimuli for reverse correlation experiments.\n",
    "* `Mediapipe` uses [mediapipe](https://google.github.io/mediapipe/)'s Face Mesh API to introduce random or precomputed deformation in the expression of a visage on an image. This engine was designed to produce batches of deformed faces for reverse correlation experiments.\n",
    "\n",
    "CLEESE is a free, standalone Python module, distributed under an open-source MIT Licence on the IRCAM Forumnet plateform. It was designed by Juan José Burred, Emmanuel Ponsot and Jean-Julien Aucouturier (STMS, IRCAM/CNRS/Sorbonne Université, Paris), with collaboration from Pascal Belin (Institut des Neurosciences de la Timone, Aix-Marseille Université), with generous funding from the European Research Council (CREAM 335536, 2014-2019, PI: JJ Aucouturier), and support for face deformation was added by Lara Kermarec (2022).\n",
    "\n",
    "This notebook is a short tutorial on how to use CLEESE to manipulate sounds, and is a companion to the PDF documentation available [here](https://github.com/creamlab/cleese/raw/master/doc/CLEESE_manual_v2.0.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Installation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Python / Jupyter Notebook\n",
    "\n",
    "CLEESE being a Python package, you will to first have a working installation of Python (versions 2.7 or 3). In addition, to run this tutorial, you will need Jupyter Notebook as well as a number of commonly used packages for scientific computing. For new users, we highly recommend [installing Anaconda](https://www.continuum.io/downloads)>. Anaconda conveniently installs Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.\n",
    "\n",
    "Use the following installation steps:\n",
    "* Download [Anaconda](https://www.continuum.io/downloads). We recommend downloading Anaconda’s  Python 2.7 version.\n",
    "* Install the version of Anaconda which you downloaded, following the instructions on the download page.\n",
    "\n",
    "#### 2. CLEESE\n",
    "\n",
    "* Install CLEESE from your shell/command line: ```pip install cleese-stim```\n",
    "* Launch Jupyter notebook from your shell/command line ```jupyter notebook``` and navigate to this tutorial .ipynb file\n",
    "* Try running the following cell, as a test. It should import cleese as well as a number of scientific packages included in the Anaconda distribution, and return with no error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleese_stim as cleese\n",
    "from cleese_stim.engines import PhaseVocoder\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display, Audio\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 5, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PYSPTK\n",
    "\n",
    "While CLEESE is standalone and does not require any further packages to be work, this tutorial uses extra functions from the [Python Speech Processing ToolKit (pysptk)](http://pysptk.readthedocs.io/en/latest/), to extract and visualize the pitch of the audio files manipulated here. \n",
    "\n",
    "* If you're on a Mac, you might need to install the Xcode Command Line Tools before installing pysptk. To do so, run ```xcode-select --install``` on the terminal.\n",
    "* Install pysptk by running ```pip install pysptk```, or alternatively follow the pysptk installation instructions [here](http://pysptk.readthedocs.io/en/latest/introduction.html)\n",
    "* Run the following cell for a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysptk\n",
    "PITCH_BOUNDS=[50,400]\n",
    "PITCH_WIN = .001 #s\n",
    "def extract_pitch(x, sr, win=PITCH_WIN, bounds=PITCH_BOUNDS): \n",
    "    hop_size = np.floor(sr * win)\n",
    "    min_f0, max_f0 = bounds\n",
    "    pitch = pysptk.swipe(x.astype(np.float64), fs=sr, hopsize=hop_size, min=min_f0, max=max_f0, otype=1)\n",
    "    times = 1000*np.arange(len(pitch))*hop_size/sr\n",
    "    return np.array(pitch), times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Basic sound manipulation with CLEESE </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random pitch profile in a single utterance </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic usage scenario of CLEESE is to input a single recording (ex. the French word \"vraiment\" - \"really\", recorded by a single male speaker ```sounds/male_vraiment_flattened.wav```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/male_vraiment_flattened.wav\"\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "Audio(data=wave_in, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use CLEESE to transform the sound with a random pitch profile. This, like all cleese operations, is done by passing to the main cleese function `cleese.process_data` a configuration file which specifies the manipulation we want. Here: cut the file in `pitch.window.count = 6` time segments, draw a random pitch shift factor at each segment boundary from a Gaussian distribution centered on 0 and standard deviation `pitch.std = 300`cents, and interpolate between segment boundaries using linear `pitch.BPFType = \"ramp\"`. (See PDF documentation for more information)\n",
    "\n",
    "```toml\n",
    "[pitch]\n",
    "# pitch transposition window in seconds. If 0 : static transformation\n",
    "window.len = 0.11\n",
    "\n",
    "# number of pitch transposition windows. If 0 : static transformation\n",
    "window.count = 6\n",
    "\n",
    "# 's': force winlength in seconds,'n': force number of windows (equal length)\n",
    "window.unit = 'n'\n",
    "\n",
    "# standard deviation (cents) for random transposisiton (Gaussian distrib for now)\n",
    "std = 300\n",
    "\n",
    "# truncate distribution values (factor of std)\n",
    "trunc = 1\n",
    "\n",
    "# type of breakpoint function:\n",
    "#      'ramp': linear interpolation between breakpoints\n",
    "#      'square': square BPF, with specified transition times at edges\n",
    "BPFtype = 'ramp'\n",
    "\n",
    "# in s: transition time for square BPF\n",
    "trTime = 0.02\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/male_vraiment_flattened.wav\"\n",
    "config_file = \"tutorial/configs/random_pitch_profile.toml\"\n",
    "\n",
    "# read input wavefile, and extract pitch for display (unnecessary for cleese.process below)\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "display(Markdown(\"Original file:\"))\n",
    "Audio(data=wave_in, rate=sr)\n",
    "pitch,times = extract_pitch(wave_in,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()\n",
    "\n",
    "# CLEESE\n",
    "wave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr)\n",
    "\n",
    "# display transformed file\n",
    "display(Markdown(\"Transformed file:\"))\n",
    "Audio(data=wave_out, rate=sr)\n",
    "pitch,times = extract_pitch(wave_out,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random speed profile in a song </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEESE can process longer files than a single word and, instead of manipulating pitch, can manipulate the duration of each portion of the file. To demonstrate this, we use CLEESE to randomly stretch each note in a recording of a song (the French song \"Joyeux Anniversaire\" / \"Happy Birthday\", sung by a female singer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/female_anniversaire_isochrone.wav\"\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "Audio(data=wave_in, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, as above, is done by passing to `cleese.process_data` a configuration file which specifies the manipulation we want. Here: cut the file in `stretch.window.len = 0.5` second time segments, draw a random stretch shift factor at each segment boundary from a Gaussian distribution centered on 1.0 and standard deviation `stretch.std = 1.5` (where factors >1 correspond to a time stretch, and factors <1 correspond to a time compression), and interpolate between segment boundaries using linear `stretch.BPFType = \"ramp\"`. (See PDF documentation for more information)\n",
    "\n",
    "```toml\n",
    "[stretch]\n",
    "\n",
    "window.len = 0.1\n",
    "window.count = 5\n",
    "window.unit = 'n'\n",
    "\n",
    "# stretching factor. >1: expansion, <1: compression\n",
    "std = 1.5\n",
    "trunc = 1\n",
    "BPFtype = 'ramp'\n",
    "trTime = 0.05\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/female_anniversaire_isochrone.wav\"\n",
    "config_file = \"tutorial/configs/random_speed_profile.toml\"\n",
    "\n",
    "# read input wavefile, and extract pitch for display (unnecessary for cleese.process below)\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "display(Markdown(\"Original file:\"))\n",
    "Audio(data=wave_in, rate=sr)\n",
    "pitch,times = extract_pitch(wave_in,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()\n",
    "\n",
    "# CLEESE\n",
    "wave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr)\n",
    "\n",
    "# display transformed file\n",
    "display(Markdown(\"Transformed file:\"))\n",
    "Audio(data=wave_out, rate=sr)\n",
    "pitch,times = extract_pitch(wave_out,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Batched transforms </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating output files one at a time, CLEESE can be used to generate large numbers of manipulated files, each randomly generated using parameters specified in config files as above. This is achieve by pusing cleese.generate_stimuli `cleese.generate_stimuli(PhaseVocoder, input_file, config_file)`. Output files are not returned by the function, but directly written in `main.outPath`, and the number of output files generated is given by `main.numFiles`, all of which are found in the configuration file:\n",
    "\n",
    "```toml\n",
    "[main]\n",
    "\n",
    "# output root folder\n",
    "outPath = \"./output/\"\n",
    "\n",
    "# number of output files to generate (for random modifications)\n",
    "numFiles = 10\n",
    "\n",
    "# apply transformation in series (True) or parallel (False)\n",
    "chain = true\n",
    "\n",
    "# transformations to apply\n",
    "transf = [\"pitch\"]\n",
    "\n",
    "# generate experiment folder with name based on current time\n",
    "generateExpFolder = true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/male_vraiment_flattened.wav\"\n",
    "config_file = \"tutorial/configs/random_pitch_profile.toml\"\n",
    "\n",
    "# CLEESE\n",
    "cleese.generate_stimuli(PhaseVocoder, input_file, config_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Chained transforms </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEESE can process files with a series of transformations that follow each other, e.g. first time-stretch the file, then pitch-shift it. This is done by specifying keyword `chain = true` under the configuration section `[main]`, as well as the list of transformations to be applied, e.g. here `transf = ['pitch','stretch']`.  \n",
    "\n",
    "```toml\n",
    "[main]\n",
    "\n",
    "# output root folder\n",
    "outPath = \"./output/\"\n",
    "\n",
    "# number of output files to generate (for random modifications)\n",
    "numFiles = 10\n",
    "\n",
    "# apply transformation in series (True) or parallel (False)\n",
    "chain = true\n",
    "\n",
    "# transformations to apply\n",
    "transf = [\"pitch\", \"stretch\"]\n",
    "\n",
    "# generate experiment folder with name based on current time\n",
    "generateExpFolder = true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/male_vraiment_flattened.wav\"\n",
    "config_file = \"tutorial/configs/chained_pitch_stretch.toml\"\n",
    "\n",
    "# CLEESE\n",
    "cleese.generate_stimuli(PhaseVocoder, input_file, config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Advanced use </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Flattening files </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying CLEESE to generate stimuli for reverse correlation, it is often advisable to use base stimuli that are as flat as possible (e.g., if randomizing pitch, start with a sound that has constant pitch). CLEESE can be used to flatten an existing recording, using the trick of not letting the tool generate its own random breakpoint function, but rather providing it with a custom function that inverts the natural pitch variations found in the original file. We demonstrate this with an original, non flattened recording of the word \"vraiment\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/male_vraiment_original.wav\"\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "Audio(data=wave_in, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has a soft, down-ward pitch contour, as show in the analysis below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse original file\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "display(Markdown(\"Original file:\"))\n",
    "pitch,times = extract_pitch(wave_in,sr, win=0.005, bounds=[50, 200])\n",
    "# display original file\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flatten this existing contour, we construct a custom breakpoint function that passes through the pitch shift values needed to shift the contour down to a constant pitch value, arbitrarily set here at 100Hz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pitch = 110.\n",
    "def difference_to_cents(pitch, ref_pitch):\n",
    "    if pitch >0:\n",
    "        return -1200*np.log2(pitch/ref_pitch)\n",
    "    else:\n",
    "        return 1\n",
    "bpf_times = times/1000\n",
    "bpf_val = np.array([difference_to_cents(hz, mean_pitch) for hz in pitch])\n",
    "# display original file\n",
    "_=plt.plot(1000*bpf_times, bpf_val, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('BPF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply this custom BPF to the original file, using `cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)` (passing audio data as input, because we don't need batch mode here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"tutorial/configs/random_pitch_profile.toml\"\n",
    "\n",
    "# CLEESE\n",
    "bpf = np.column_stack((bpf_times,bpf_val))\n",
    "wave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)\n",
    "\n",
    "# display transformed file\n",
    "display(Markdown(\"Transformed file:\"))\n",
    "Audio(data=wave_out, rate=sr)\n",
    "pitch_f,times_f = extract_pitch(wave_out,sr, win=0.005, bounds=[50, 200])\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.plot(times_f, pitch_f, 'b')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Using custom breakpoints (1) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating linearly spaced time windows (or, as called here, breakpoints), CLEESE supports a list of externally provided time positions. To demonstrate this, we use CLEESE to stretch the duration of each note in the song \"Joyeux Anniversaire\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/female_anniversaire_isochrone.wav\"\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "Audio(data=wave_in, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find note boundaries, we can e.g. use an external audio editor such as <a href=\"https://www.audacityteam.org\"> Audacity</a>, and measure time positions between notes as ```[0.027, 0.634, 1.137, 1.647, 2.185, 2.649, 3.181]```. \n",
    "<img src=\"tutorial/pics/audacity.png\" alt=\"Audacity\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generate a breakpoint function with `cleese.create_BPF` which uses these time points and parameters loaded from the stretch config file `config_file`. This BPF can then be passed to `cleese.process_data` as argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/female_anniversaire_isochrone.wav\"\n",
    "config_file = \"tutorial/configs/random_speed_profile.toml\"\n",
    "\n",
    "# display original file\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "display(Markdown(\"Original file:\"))\n",
    "Audio(data=wave_in, rate=sr)\n",
    "pitch,times = extract_pitch(wave_in,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()\n",
    "\n",
    "# CLEESE\n",
    "time_points = np.array([0.027, 0.634, 1.137, 1.647, 2.185, 2.649, 3.181]) # values found in audacity\n",
    "num_points = len(time_points)\n",
    "bpf = PhaseVocoder.create_BPF(\n",
    "    'stretch',config_file,time_points,num_points,0)   \n",
    "wave_out,bpf_out = cleese.process_data(\n",
    "    PhaseVocoder, wave_in, config_file, sample_rate=sr, BPF=bpf)\n",
    "\n",
    "# display transformed file\n",
    "display(Markdown(\"Transformed file:\"))\n",
    "Audio(data=wave_out, rate=sr)\n",
    "pitch,times = extract_pitch(wave_out,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Using custom breakpoints (2) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how to pass fixed breakpoints to both a stretch and a pitch transform, thus generating a random melody of notes that have both random duration and random pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"tutorial/sounds/female_anniversaire_isochrone.wav\"\n",
    "config_file_pitch = \"tutorial/configs/random_pitch_profile.toml\"\n",
    "config_file_stretch = \"tutorial/configs/random_speed_profile.toml\"\n",
    "\n",
    "# display original file\n",
    "wave_in, sr, _ = PhaseVocoder.wavRead(input_file)\n",
    "display(Markdown(\"Original file:\"))\n",
    "Audio(data=wave_in, rate=sr)\n",
    "pitch,times = extract_pitch(wave_in,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()\n",
    "\n",
    "# CLEESE\n",
    "time_points = np.array([0.027, 0.634, 1.137, 1.647, 2.185, 2.649, 3.181]) # values found in audacity\n",
    "num_points = len(time_points)\n",
    "bpf_stretch = PhaseVocoder.create_BPF(\n",
    "    'stretch',config_file_stretch,time_points,num_points,0)   \n",
    "# print bpf_stretch\n",
    "wave_out,bpf_out = cleese.process_data(\n",
    "        PhaseVocoder,\n",
    "        wave_in,\n",
    "        config_file_stretch,\n",
    "        sample_rate=sr,\n",
    "        BPF=bpf_stretch)\n",
    "\n",
    "# display transformed file\n",
    "display(Markdown(\"Stretched file:\"))\n",
    "Audio(data=wave_out, rate=sr)\n",
    "pitch,times = extract_pitch(wave_out,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()\n",
    "   \n",
    "bpf_pitch = PhaseVocoder.create_BPF(\n",
    "    'pitch',config_file_pitch,time_points,num_points,0)   \n",
    "# print bpf_pitch\n",
    "wave_out,bpf_out = cleese.process_data(PhaseVocoder, wave_out, config_file_pitch, sample_rate=sr, BPF=bpf_pitch)\n",
    "\n",
    "# display transformed file\n",
    "display(Markdown(\"Pitched file:\"))\n",
    "Audio(data=wave_out, rate=sr)\n",
    "pitch,times = extract_pitch(wave_out,sr)\n",
    "_=plt.plot(times, pitch, 'k')\n",
    "_=plt.xlabel('time in file (ms)')\n",
    "_=plt.ylabel('pitch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
